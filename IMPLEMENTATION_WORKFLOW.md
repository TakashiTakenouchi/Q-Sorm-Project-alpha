# Q-Storm Platform - Agile Implementation Workflow

**Workflow Version**: 1.0
**Strategy**: Agile (2-week sprints)
**Duration**: 18 weeks (9 sprints)
**Team Size**: 4-6 (Backend, Frontend, DevOps, QA)
**Generated**: 2025-10-18
**Based on**: SYSTEM_ARCHITECTURE_SPECIFICATION.md v3.0

---

## Executive Summary

### Implementation Overview

This workflow provides a comprehensive, sprint-based implementation plan for the Q-Storm Platform, transforming the current state (app.py v2.0 and app_improved.py v3.0 with partial features) into a production-ready AI-powered QC Story analysis system with complete backend APIs, integrated frontend, persistent storage, and intelligent analysis capabilities.

### Key Deliverables

| Phase | Duration | Primary Deliverable | Production Status |
|-------|----------|-------------------|------------------|
| **Phase 1 - Foundation** | Weeks 1-6 (Sprints 1-3) | Complete API coverage, integrated frontend, security hardening | âœ… Production v1.0 |
| **Phase 2 - Infrastructure** | Weeks 7-12 (Sprints 4-6) | PostgreSQL persistence, Redis caching, Celery background tasks, JWT auth | âœ… Production v2.0 |
| **Phase 3 - Intelligence** | Weeks 13-18 (Sprints 7-9) | AI agent integration (ç¾çŠ¶æŠŠæ¡, åŸå› ç‰¹å®š, åŠ¹æœäºˆæ¸¬), knowledge system | âœ… Production v3.0 |

### Success Metrics

| Metric | Phase 1 Target | Phase 2 Target | Phase 3 Target |
|--------|---------------|---------------|---------------|
| **API Coverage** | 100% (all endpoints implemented) | 100% + background processing | 100% + AI insights |
| **Response Time** | <5s (10 concurrent users) | <3s (50 concurrent users) | <3s with AI analysis |
| **Uptime** | 99% | 99.5% | 99.9% |
| **User Satisfaction** | 80% | 85% | 90% |
| **Test Coverage** | >70% | >80% | >85% |

---

## Phase 1: EDAæ©Ÿèƒ½ + LangChainçµ±åˆ (Weeks 1-6)

**âš ï¸ é‡è¦**: Phase 1ã®ã¿å®Ÿè£…ã‚’è¡Œã„ã€Phase 2ï¼ˆåŸå› ç‰¹å®šæ©Ÿèƒ½ï¼‰ã¯ç€æ‰‹ã—ãªã„ã§ãã ã•ã„ã€‚

**Goal**: EDAãƒ¢ãƒ¼ãƒ‰é¸æŠæ©Ÿèƒ½ã¨LangChainçµ±åˆã«ã‚ˆã‚‹æ—¥æœ¬èªæ´å¯Ÿç”Ÿæˆã®å®Ÿè£…

**Phase 1 Success Criteria**:
- âœ… ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒã€Œè‡ªå‹•EDAã€ã¨ã€Œãƒãƒ‹ãƒ¥ã‚¢ãƒ«åˆ†æã€ã‚’é¸æŠå¯èƒ½
- âœ… Sweetvizã«ã‚ˆã‚‹åŒ…æ‹¬çš„ãªEDAãƒ¬ãƒãƒ¼ãƒˆè‡ªå‹•ç”Ÿæˆ  
- âœ… LangChainã«ã‚ˆã‚‹æ—¥æœ¬èªãƒ“ã‚¸ãƒã‚¹æ´å¯Ÿã®è‡ªå‹•ç”Ÿæˆ
- âœ… OpenAI APIã‚¨ãƒ©ãƒ¼æ™‚ã®ã‚°ãƒ¬ãƒ¼ã‚¹ãƒ•ãƒ«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ™ãƒ¼ã‚¹ï¼‰
- âœ… ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã‹ã‚‰ã®EDAå®Ÿè¡Œã¨ãƒ¬ãƒãƒ¼ãƒˆè¡¨ç¤º
- âœ… å˜ä½“ãƒ†ã‚¹ãƒˆãƒ»çµ±åˆãƒ†ã‚¹ãƒˆã®å®Œå‚™

### Sprint 1: ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ EDAãƒ¢ãƒ¼ãƒ‰é¸æŠæ©Ÿèƒ½ (Week 1-2)

**Sprint Goal**: åˆ†æãƒ¢ãƒ¼ãƒ‰é¸æŠAPI + Sweetvizçµ±åˆ + çµ±è¨ˆã‚µãƒãƒªãƒ¼ç”Ÿæˆ

**Total Story Points**: 18

#### User Stories

**US1.1 - åˆ†æãƒ¢ãƒ¼ãƒ‰é¸æŠã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ** (Story Points: 5)

```
As a data analyst
I want to select between "Auto EDA" and "Manual Analysis" modes
So that I can choose the appropriate analysis approach for my workflow
```

**Acceptance Criteria**:
- âœ… Endpoint `POST /api/v1/analysis/mode` implemented (Section 4.2.6)
- âœ… Validates mode parameter (`"auto_eda"` or `"manual"`)
- âœ… Saves configuration to `uploads/<session_id>/analysis_config.json`
- âœ… Returns success response with next step guidance
- âœ… Error handling for invalid modes and missing session

**Implementation Tasks**:
```python
# app_improved.py (maintain single-file architecture)

@app.route('/api/v1/analysis/mode', methods=['POST'])
def save_analysis_mode():
    """
    Save user's analysis mode selection
    Request: {session_id, mode, target_column?, options?}
    Response: {status, config_saved, config_path, next_step}
    """
    # Task 1.1.1: Validate request parameters
    # Task 1.1.2: Validate session_id exists
    # Task 1.1.3: Create analysis_config.json with mode, timestamp, options
    # Task 1.1.4: Return success with next_step guidance
    
    data = request.get_json()
    session_id = data.get('session_id')
    mode = data.get('mode')
    
    if mode not in ['auto_eda', 'manual']:
        return jsonify({
            'status': 'error',
            'code': 'INVALID_MODE',
            'message': 'ç„¡åŠ¹ãªåˆ†æãƒ¢ãƒ¼ãƒ‰ã§ã™',
            'allowed_values': ['auto_eda', 'manual']
        }), 400
    
    config_path = os.path.join('uploads', session_id, 'analysis_config.json')
    config = {
        'mode': mode,
        'target_column': data.get('target_column'),
        'timestamp': datetime.now().isoformat(),
        'options': data.get('options', {})
    }
    
    with open(config_path, 'w') as f:
        json.dump(config, f, indent=2)
    
    return jsonify({
        'status': 'success',
        'session_id': session_id,
        'mode': mode,
        'config_saved': True,
        'config_path': f'/uploads/{session_id}/analysis_config.json',
        'next_step': {
            'endpoint': '/api/v1/analysis/eda/execute',
            'method': 'POST',
            'description': 'EDAåˆ†æã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„'
        }
    })
```

**Testing Requirements**:
- Unit test: `test_analysis_mode_endpoint()` validates mode selection and config saving
- Integration test: Upload data â†’ select mode â†’ verify config file created
- Error test: Invalid mode returns 400 with proper error message

**Dependencies**: None (uses existing session storage)

---

**US1.2 - Sweetvizçµ±åˆ** (Story Points: 8)

```
As a data analyst
I want automated EDA reports generated with Sweetviz
So that I can quickly understand data distributions and relationships
```

**Acceptance Criteria**:
- âœ… Sweetviz library integrated and configured
- âœ… Function `execute_sweetviz_analysis(df, session_id, target_column)` implemented
- âœ… HTML report generated in `outputs/<session_id>/eda_report.html`
- âœ… Handles target column specification (optional)
- âœ… Execution completes within 30 seconds for typical datasets (<10,000 rows)
- âœ… Error handling for Sweetviz failures

**Implementation Tasks**:
```python
import sweetviz as sv

def execute_sweetviz_analysis(df: pd.DataFrame, session_id: str, 
                               target_column: str = None) -> str:
    """
    Execute Sweetviz EDA analysis
    
    Args:
        df: DataFrame to analyze
        session_id: Session ID for output path
        target_column: Optional target feature for supervised analysis
        
    Returns:
        Path to generated HTML report
    """
    # Task 1.2.1: Create outputs directory if not exists
    output_dir = os.path.join('outputs', session_id)
    os.makedirs(output_dir, exist_ok=True)
    
    # Task 1.2.2: Generate Sweetviz report
    try:
        if target_column and target_column in df.columns:
            report = sv.analyze(df, target_feat=target_column)
        else:
            report = sv.analyze(df)
        
        # Task 1.2.3: Save HTML report
        report_path = os.path.join(output_dir, 'eda_report.html')
        report.show_html(filepath=report_path, open_browser=False)
        
        logger.info(f'Sweetviz report generated: {report_path}')
        return report_path
        
    except Exception as e:
        logger.error(f'Sweetviz analysis failed: {e}', exc_info=True)
        raise
```

**Testing Requirements**:
- Unit test: `test_sweetviz_execution()` generates report for test DataFrame
- Integration test: Real store data â†’ Sweetviz â†’ verify HTML file exists and has content
- Performance test: 5,000 rows complete within 30 seconds
- Error test: Invalid target column handled gracefully

**Dependencies**:
- `pip install sweetviz==2.3.1`
- Requires matplotlib, pandas (already installed)

---

**US1.3 - çµ±è¨ˆã‚µãƒãƒªãƒ¼ç”Ÿæˆ** (Story Points: 5)

```
As a data analyst
I want key statistical summaries extracted from my data
So that I can quickly identify important numeric patterns
```

**Acceptance Criteria**:
- âœ… Function `generate_stats_summary(df)` returns mean, std, min, max, median, quartiles
- âœ… Function `extract_high_correlations(df, threshold=0.7)` returns correlated pairs
- âœ… Function `detect_outliers_count(df)` counts outliers using IQR method
- âœ… All numeric columns processed
- âœ… Results returned as JSON-safe dictionary

**Implementation Tasks**:
```python
def generate_stats_summary(df: pd.DataFrame) -> dict:
    """Generate statistical summary for numeric columns"""
    # Task 1.3.1: Select numeric columns
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    
    stats = {}
    for col in numeric_cols:
        stats[col] = {
            'mean': float(df[col].mean()),
            'std': float(df[col].std()),
            'min': float(df[col].min()),
            'max': float(df[col].max()),
            'median': float(df[col].median()),
            'q25': float(df[col].quantile(0.25)),
            'q75': float(df[col].quantile(0.75)),
            'missing': int(df[col].isnull().sum())
        }
    
    return stats

def extract_high_correlations(df: pd.DataFrame, threshold: float = 0.7) -> list:
    """Extract high correlation pairs"""
    # Task 1.3.2: Calculate correlation matrix
    corr_matrix = df.select_dtypes(include=[np.number]).corr()
    
    high_corrs = []
    for i in range(len(corr_matrix.columns)):
        for j in range(i+1, len(corr_matrix.columns)):
            corr_value = corr_matrix.iloc[i, j]
            if abs(corr_value) > threshold:
                high_corrs.append({
                    'col1': corr_matrix.columns[i],
                    'col2': corr_matrix.columns[j],
                    'correlation': round(float(corr_value), 3)
                })
    
    return high_corrs

def detect_outliers_count(df: pd.DataFrame) -> dict:
    """Count outliers using IQR method"""
    # Task 1.3.3: Detect outliers for each numeric column
    outlier_counts = {}
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    
    for col in numeric_cols:
        Q1 = df[col].quantile(0.25)
        Q3 = df[col].quantile(0.75)
        IQR = Q3 - Q1
        outliers = df[(df[col] < Q1 - 1.5*IQR) | (df[col] > Q3 + 1.5*IQR)]
        if len(outliers) > 0:
            outlier_counts[col] = len(outliers)
    
    return outlier_counts
```

**Testing Requirements**:
- Unit test: `test_stats_summary()` validates summary calculations
- Unit test: `test_high_correlations()` detects known correlations
- Unit test: `test_outlier_detection()` counts outliers correctly
- Edge case test: Empty DataFrame, single column, all NaN values

**Dependencies**: None (uses pandas, numpy already installed)

---

**Sprint 1 Definition of Done**:
- [ ] All 3 user stories completed and tested
- [ ] Code review completed (maintain single-file architecture)
- [ ] Unit tests pass with >70% coverage
- [ ] Integration tests pass
- [ ] Documentation updated in CLAUDE.md
- [ ] No regressions in existing Pareto analysis functionality

---

### Sprint 2: LangChainçµ±åˆ (Week 3-4)

**Sprint Goal**: LangChain + OpenAI GPT-3.5-turbo integration for Japanese business insights generation with fallback

**Total Story Points**: 21

#### User Stories

**US2.1 - LangChainåŸºç›¤ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—** (Story Points: 3)

```
As a developer
I want LangChain properly configured with OpenAI API
So that we can generate natural language insights
```

**Acceptance Criteria**:
- âœ… Dependencies installed: `langchain==0.1.0`, `openai==1.12.0`
- âœ… Environment variable `OPENAI_API_KEY` validation at startup
- âœ… Function `validate_openai_api_key()` returns (bool, str) tuple
- âœ… API key masking in logs (sk-xxx***xxx)
- âœ… LangChain ChatOpenAI model initialized with proper config

**Implementation Tasks**:
```python
from langchain.chat_models import ChatOpenAI
from langchain.schema import HumanMessage
import os
import re

def validate_openai_api_key() -> tuple[bool, str]:
    """Validate OpenAI API key configuration"""
    # Task 2.1.1: Check environment variable
    api_key = os.environ.get('OPENAI_API_KEY', '')
    
    if not api_key:
        return False, 'OPENAI_API_KEYç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“'
    
    # Task 2.1.2: Validate format
    if not api_key.startswith('sk-'):
        return False, 'APIã‚­ãƒ¼ã®å½¢å¼ãŒä¸æ­£ã§ã™ï¼ˆsk-ã§å§‹ã¾ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ï¼‰'
    
    if len(api_key) < 20:
        return False, 'APIã‚­ãƒ¼ãŒçŸ­ã™ãã¾ã™'
    
    # Task 2.1.3: Log with masking
    masked_key = api_key[:7] + '*' * (len(api_key) - 11) + api_key[-4:]
    logger.info(f'OpenAI APIã‚­ãƒ¼æ¤œè¨¼æˆåŠŸ: {masked_key}')
    
    return True, 'OK'

# Initialize at app startup
def init_langchain():
    """Initialize LangChain components"""
    is_valid, message = validate_openai_api_key()
    
    if not is_valid:
        logger.warning(f'LangChainæ©Ÿèƒ½ãŒç„¡åŠ¹åŒ–ã•ã‚Œã¾ã™: {message}')
        return None
    
    try:
        llm = ChatOpenAI(
            model_name="gpt-3.5-turbo",
            temperature=0.7,
            openai_api_key=os.environ.get('OPENAI_API_KEY'),
            request_timeout=30
        )
        logger.info('LangChain ChatOpenAI initialized successfully')
        return llm
    except Exception as e:
        logger.error(f'LangChain initialization failed: {e}')
        return None
```

**Testing Requirements**:
- Unit test: `test_api_key_validation()` tests valid/invalid keys
- Integration test: Real API key â†’ successful LLM initialization
- Error test: Missing/invalid key â†’ proper warning and None return

**Dependencies**:
```bash
pip install langchain==0.1.0 openai==1.12.0
```

---

**US2.2 - ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆè¨­è¨ˆ** (Story Points: 5)

```
As a system designer
I want well-structured prompt templates for EDA insights
So that we generate consistent, high-quality Japanese explanations
```

**Acceptance Criteria**:
- âœ… Class `PromptTemplateManager` with `EDA_INSIGHTS_TEMPLATE` and `PARETO_EXPLANATION_TEMPLATE`
- âœ… Template includes dataset info, stats summary, correlations, outliers
- âœ… Variables dynamically injected via `.format()`
- âœ… Output format guides LLM to structured Japanese response
- âœ… Templates tested with mock data

**Implementation** (see SYSTEM_ARCHITECTURE_SPECIFICATION.md Section 8.3.3 for full code)

**Testing Requirements**:
- Unit test: `test_prompt_template_building()` validates variable injection
- Unit test: `test_stats_formatting()` checks formatted output
- Unit test: `test_correlation_extraction()` verifies correlation pairs
- Edge case test: Empty DataFrame, no correlations, no outliers

**Dependencies**: None (pure Python string formatting)

---

**US2.3 - EDAæ´å¯Ÿç”Ÿæˆé–¢æ•°** (Story Points: 8)

```
As a data analyst
I want LangChain to generate Japanese business insights from EDA
So that I can understand data patterns in business context
```

**Acceptance Criteria**:
- âœ… Class `LangChainInsightGenerator` with `generate_eda_insights(df, stats_summary)` method
- âœ… Returns dict with `{insights, source, model, warning}`
- âœ… LRU cache (max 100 entries, TTL 1 hour) for API responses
- âœ… Retry logic (max 3 attempts) with exponential backoff
- âœ… Automatic fallback to `_generate_template_insights()` on API error
- âœ… Warning message when using template fallback

**Implementation** (see SYSTEM_ARCHITECTURE_SPECIFICATION.md Section 8.3.4 for full code)

**Testing Requirements**:
- Unit test: `test_langchain_insights_generation()` with mocked LLM
- Unit test: `test_cache_hit()` validates caching behavior
- Unit test: `test_api_error_fallback()` verifies template fallback
- Unit test: `test_retry_logic()` confirms 3-attempt retry
- Integration test: Real API â†’ Japanese insights generation

**Dependencies**: LangChain, OpenAI API (from US2.1)

---

**US2.4 - EDAå®Ÿè¡Œã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ** (Story Points: 5)

```
As a data analyst
I want a single endpoint to execute EDA with Sweetviz and LangChain
So that I can get comprehensive analysis results in one API call
```

**Acceptance Criteria**:
- âœ… Endpoint `POST /api/v1/analysis/eda/execute` implemented (Section 4.2.7)
- âœ… Integrates Sweetviz execution + LangChain insights generation
- âœ… Returns `{status, report_path, insights, key_findings, stats_summary, execution_time}`
- âœ… Handles optional filters (shop, start_date, end_date, remove_outliers)
- âœ… Graceful error handling with proper HTTP status codes

**Implementation**:
```python
class EDAAnalysisEngine:
    """EDA analysis engine integrating Sweetviz + LangChain"""
    
    def __init__(self, session_id: str):
        self.session_id = session_id
        self.upload_dir = os.path.join('uploads', session_id)
        self.output_dir = os.path.join('outputs', session_id)
        self.langchain_generator = LangChainInsightGenerator()
        os.makedirs(self.output_dir, exist_ok=True)
    
    def execute_eda(self, df: pd.DataFrame, target_column: str = None) -> dict:
        """Execute complete EDA analysis"""
        start_time = time.time()
        
        try:
            # Step 1: Sweetviz report
            report = sv.analyze(df, target_feat=target_column) if target_column else sv.analyze(df)
            report_path = os.path.join(self.output_dir, 'eda_report.html')
            report.show_html(filepath=report_path, open_browser=False)
            
            # Step 2: Stats summary
            stats_summary = generate_stats_summary(df)
            
            # Step 3: LangChain insights
            insights_result = self.langchain_generator.generate_eda_insights(df, stats_summary)
            
            # Step 4: Key findings
            key_findings = self._extract_key_findings(df, stats_summary)
            
            execution_time = time.time() - start_time
            
            return {
                'status': 'success',
                'report_path': f'/outputs/{self.session_id}/eda_report.html',
                'insights': insights_result['insights'],
                'insights_source': insights_result['source'],
                'insights_model': insights_result['model'],
                'warning': insights_result.get('warning'),
                'key_findings': key_findings,
                'stats_summary': stats_summary,
                'execution_time': round(execution_time, 2)
            }
        except Exception as e:
            logger.error(f'EDA execution error: {e}', exc_info=True)
            return {
                'status': 'error',
                'error': str(e),
                'execution_time': round(time.time() - start_time, 2)
            }

@app.route('/api/v1/analysis/eda/execute', methods=['POST'])
def execute_eda_analysis():
    """EDA execution endpoint"""
    data = request.get_json()
    session_id = data['session_id']
    
    # Load session data
    df = load_session_data(session_id)
    
    # Apply filters if provided
    if 'filters' in data:
        df = apply_filters(df, data['filters'])
    
    # Execute EDA
    engine = EDAAnalysisEngine(session_id)
    result = engine.execute_eda(df, data.get('target_column'))
    
    if result['status'] == 'error':
        return jsonify(result), 500
    
    return jsonify(result)
```

**Testing Requirements**:
- Integration test: Upload â†’ mode selection â†’ EDA execution â†’ validate response structure
- Integration test: With LangChain mock â†’ verify insights generation
- Integration test: Without API key â†’ verify template fallback
- Performance test: 5,000 rows complete within 30 seconds
- Error test: Invalid session_id â†’ 404 response

**Dependencies**: Sweetviz (US1.2), LangChain (US2.3)

---

**Sprint 2 Definition of Done**:
- [ ] All 4 user stories completed and tested
- [ ] LangChain integration documented in CLAUDE.md
- [ ] Environment setup guide created (Appendix E)
- [ ] Unit + integration tests pass
- [ ] Performance benchmarks met (<30s EDA execution)
- [ ] Fallback mechanism validated (works without OPENAI_API_KEY)

---

### Sprint 3: ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰çµ±åˆ (Week 5-6)

**Sprint Goal**: Frontend components for mode selection, EDA execution, and results display

**Total Story Points**: 16

#### User Stories

**US3.1 - AnalysisModeSelector ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ** (Story Points: 8)

```
As a data analyst
I want a UI to select between Auto EDA and Manual Analysis modes
So that I can choose my analysis workflow before proceeding
```

**Acceptance Criteria**:
- âœ… React component `AnalysisModeSelector.tsx` created
- âœ… Radio buttons for "è‡ªå‹•EDA" and "ãƒãƒ‹ãƒ¥ã‚¢ãƒ«åˆ†æ" modes
- âœ… Target column dropdown (optional for Auto EDA)
- âœ… Analysis options checkboxes (correlations, outliers, max categories)
- âœ… "æ¬¡ã¸é€²ã‚€" button calls `/api/v1/analysis/mode` endpoint
- âœ… Success â†’ navigates to EDA execution screen or manual analysis dashboard
- âœ… Error â†’ displays error message

**Implementation**:
```typescript
// frontend/src/components/AnalysisModeSelector.tsx

import React, { useState } from 'react';
import { analysisAPI } from '../services/analysisAPI';

interface AnalysisMode {
  mode: 'auto_eda' | 'manual';
  targetColumn?: string;
  options?: {
    includeCorrelations: boolean;
    includeOutliers: boolean;
    maxCategories: number;
  };
}

export const AnalysisModeSelector: React.FC<{ sessionId: string }> = ({ sessionId }) => {
  const [mode, setMode] = useState<'auto_eda' | 'manual'>('auto_eda');
  const [targetColumn, setTargetColumn] = useState<string>('');
  const [includeCorrelations, setIncludeCorrelations] = useState(true);
  const [includeOutliers, setIncludeOutliers] = useState(true);
  const [maxCategories, setMaxCategories] = useState(30);
  const [error, setError] = useState<string | null>(null);
  const [loading, setLoading] = useState(false);

  const handleSubmit = async () => {
    setLoading(true);
    setError(null);

    try {
      const config: AnalysisMode = {
        mode,
        targetColumn: mode === 'auto_eda' ? targetColumn : undefined,
        options: {
          includeCorrelations,
          includeOutliers,
          maxCategories
        }
      };

      const response = await analysisAPI.saveMode(sessionId, config);

      if (response.status === 'success') {
        // Navigate to next step
        if (mode === 'auto_eda') {
          window.location.href = `/eda-execution?session=${sessionId}`;
        } else {
          window.location.href = `/manual-analysis?session=${sessionId}`;
        }
      }
    } catch (err: any) {
      setError(err.message || 'åˆ†æãƒ¢ãƒ¼ãƒ‰ã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ');
    } finally {
      setLoading(false);
    }
  };

  return (
    <div className="analysis-mode-selector">
      <h2>åˆ†æãƒ¢ãƒ¼ãƒ‰ã‚’é¸æŠã—ã¦ãã ã•ã„</h2>

      <div className="mode-options">
        <label>
          <input
            type="radio"
            value="auto_eda"
            checked={mode === 'auto_eda'}
            onChange={(e) => setMode('auto_eda')}
          />
          è‡ªå‹•EDAï¼ˆSweetvizï¼‰- åŒ…æ‹¬çš„ãªçµ±è¨ˆåˆ†æãƒ¬ãƒãƒ¼ãƒˆè‡ªå‹•ç”Ÿæˆ
        </label>

        <label>
          <input
            type="radio"
            value="manual"
            checked={mode === 'manual'}
            onChange={(e) => setMode('manual')}
          />
          ãƒãƒ‹ãƒ¥ã‚¢ãƒ«åˆ†æ - ãƒ‘ãƒ¬ãƒ¼ãƒˆå›³ã€æ™‚ç³»åˆ—ã€ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ã‚’å€‹åˆ¥é¸æŠ
        </label>
      </div>

      {mode === 'auto_eda' && (
        <div className="eda-options">
          <label>
            ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚«ãƒ©ãƒ ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰:
            <select value={targetColumn} onChange={(e) => setTargetColumn(e.target.value)}>
              <option value="">-- é¸æŠãªã— --</option>
              <option value="Total_Sales">Total_Sales</option>
              <option value="Operating_profit">Operating_profit</option>
              {/* ... å‹•çš„ã«åˆ—ã‚’è¿½åŠ  */}
            </select>
          </label>

          <label>
            <input
              type="checkbox"
              checked={includeCorrelations}
              onChange={(e) => setIncludeCorrelations(e.target.checked)}
            />
            é«˜ç›¸é–¢ãƒšã‚¢ã®æŠ½å‡º
          </label>

          <label>
            <input
              type="checkbox"
              checked={includeOutliers}
              onChange={(e) => setIncludeOutliers(e.target.checked)}
            />
            å¤–ã‚Œå€¤ã®æ¤œå‡º
          </label>
        </div>
      )}

      {error && <div className="error-message">{error}</div>}

      <button onClick={handleSubmit} disabled={loading}>
        {loading ? 'ä¿å­˜ä¸­...' : 'æ¬¡ã¸é€²ã‚€'}
      </button>
    </div>
  );
};
```

**API Service**:
```typescript
// frontend/src/services/analysisAPI.ts

export const analysisAPI = {
  async saveMode(sessionId: string, config: any) {
    const response = await fetch('/api/v1/analysis/mode', {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({ session_id: sessionId, ...config })
    });

    if (!response.ok) {
      const error = await response.json();
      throw new Error(error.message);
    }

    return await response.json();
  },

  async executeEDA(sessionId: string, filters?: any) {
    const response = await fetch('/api/v1/analysis/eda/execute', {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({ session_id: sessionId, filters })
    });

    if (!response.ok) {
      const error = await response.json();
      throw new Error(error.message);
    }

    return await response.json();
  }
};
```

**Testing Requirements**:
- Unit test (Jest): Component renders correctly
- Unit test: Radio button selection changes mode
- Unit test: Target column dropdown populates
- Integration test (Cypress): Full flow - upload â†’ mode selection â†’ EDA execution

**Dependencies**: React, TypeScript (assumed existing frontend)

---

**US3.2 - Dashboardçµ±åˆ** (Story Points: 5)

```
As a data analyst
I want EDA results displayed in the main dashboard
So that I can view insights alongside Pareto charts and other analyses
```

**Acceptance Criteria**:
- âœ… Component `EdaResultsPanel.tsx` displays EDA insights
- âœ… Shows Sweetviz HTML report in iframe
- âœ… Displays LangChain-generated Japanese insights (formatted markdown)
- âœ… Shows key findings as list items with severity icons
- âœ… Displays stats summary table
- âœ… "Export PDF" button (future enhancement - placeholder for now)

**Implementation**:
```typescript
// frontend/src/components/EdaResultsPanel.tsx

import React from 'react';
import ReactMarkdown from 'react-markdown';

interface EdaResult {
  status: string;
  report_path: string;
  insights: {
    source: 'langchain' | 'template';
    model: string | null;
    content: string;
    warning: string | null;
  };
  key_findings: Array<{
    type: string;
    severity?: string;
    message: string;
    details?: string[];
  }>;
  stats_summary: any;
  execution_time: number;
}

export const EdaResultsPanel: React.FC<{ result: EdaResult }> = ({ result }) => {
  return (
    <div className="eda-results-panel">
      <h2>ğŸ“Š EDA Analysis Results</h2>

      {/* LangChain Insights */}
      <section className="insights-section">
        <h3>ğŸ’¡ ãƒ“ã‚¸ãƒã‚¹æ´å¯Ÿ ({result.insights.source === 'langchain' ? 'AIç”Ÿæˆ' : 'ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ'})</h3>
        {result.insights.warning && (
          <div className="warning-banner">âš ï¸ {result.insights.warning}</div>
        )}
        <div className="markdown-content">
          <ReactMarkdown>{result.insights.content}</ReactMarkdown>
        </div>
      </section>

      {/* Key Findings */}
      <section className="key-findings-section">
        <h3>ğŸ” ä¸»è¦ç™ºè¦‹äº‹é …</h3>
        <ul>
          {result.key_findings.map((finding, idx) => (
            <li key={idx} className={`finding-${finding.severity || 'info'}`}>
              {finding.severity === 'warning' && 'âš ï¸ '}
              {finding.message}
              {finding.details && (
                <ul>
                  {finding.details.map((detail, dIdx) => (
                    <li key={dIdx}>{detail}</li>
                  ))}
                </ul>
              )}
            </li>
          ))}
        </ul>
      </section>

      {/* Sweetviz Report */}
      <section className="report-section">
        <h3>ğŸ“„ Sweetviz EDA Report</h3>
        <iframe
          src={result.report_path}
          width="100%"
          height="800px"
          style={{ border: '1px solid #ccc' }}
          title="EDA Report"
        />
      </section>

      {/* Stats Summary Table */}
      <section className="stats-section">
        <h3>ğŸ“ˆ çµ±è¨ˆã‚µãƒãƒªãƒ¼</h3>
        <table className="stats-table">
          <thead>
            <tr>
              <th>ã‚«ãƒ©ãƒ </th>
              <th>å¹³å‡</th>
              <th>æ¨™æº–åå·®</th>
              <th>æœ€å°å€¤</th>
              <th>æœ€å¤§å€¤</th>
              <th>ä¸­å¤®å€¤</th>
              <th>æ¬ æå€¤</th>
            </tr>
          </thead>
          <tbody>
            {Object.entries(result.stats_summary).map(([col, stats]: [string, any]) => (
              <tr key={col}>
                <td>{col}</td>
                <td>{stats.mean.toFixed(2)}</td>
                <td>{stats.std.toFixed(2)}</td>
                <td>{stats.min.toFixed(2)}</td>
                <td>{stats.max.toFixed(2)}</td>
                <td>{stats.median.toFixed(2)}</td>
                <td>{stats.missing}</td>
              </tr>
            ))}
          </tbody>
        </table>
      </section>

      <div className="execution-info">
        â±ï¸ å®Ÿè¡Œæ™‚é–“: {result.execution_time.toFixed(2)}ç§’
      </div>
    </div>
  );
};
```

**Dashboard Integration**:
```typescript
// frontend/src/components/StoreAnalyticsDashboard.tsx

import { AnalysisModeSelector } from './AnalysisModeSelector';
import { EdaResultsPanel } from './EdaResultsPanel';

export const StoreAnalyticsDashboard: React.FC = () => {
  const [sessionId, setSessionId] = useState<string | null>(null);
  const [edaResult, setEdaResult] = useState<any | null>(null);

  // ... existing code ...

  return (
    <div className="dashboard">
      {!sessionId && <FileUpload onUploadSuccess={(sid) => setSessionId(sid)} />}
      {sessionId && !edaResult && <AnalysisModeSelector sessionId={sessionId} />}
      {edaResult && <EdaResultsPanel result={edaResult} />}
    </div>
  );
};
```

**Testing Requirements**:
- Unit test: Component renders with mock EDA result
- Unit test: Markdown insights displayed correctly
- Unit test: Stats table populates from summary object
- Visual test: Screenshot comparison for UI consistency

**Dependencies**: `react-markdown` for insights rendering

---

**US3.3 - ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã¨ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°çŠ¶æ…‹** (Story Points: 3)

```
As a user
I want clear feedback during EDA execution
So that I know the system is working and understand any errors
```

**Acceptance Criteria**:
- âœ… Loading spinner during EDA execution (typical 10-30 seconds)
- âœ… Progress indicator or estimated time display
- âœ… Error boundary catches frontend errors and displays user-friendly message
- âœ… Timeout handling (60 seconds) with retry option
- âœ… Network error detection and offline message

**Implementation**:
```typescript
// frontend/src/components/EdaExecutionLoader.tsx

import React, { useState, useEffect } from 'react';

export const EdaExecutionLoader: React.FC<{ sessionId: string }> = ({ sessionId }) => {
  const [progress, setProgress] = useState(0);
  const [estimatedTime, setEstimatedTime] = useState(25);
  const [error, setError] = useState<string | null>(null);

  useEffect(() => {
    const executeEDA = async () => {
      const startTime = Date.now();
      const timeout = setTimeout(() => {
        setError('ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: EDAå®Ÿè¡ŒãŒ60ç§’ã‚’è¶…ãˆã¾ã—ãŸã€‚ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºãŒå¤§ãã™ãã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚');
      }, 60000); // 60 second timeout

      try {
        // Simulate progress updates
        const progressInterval = setInterval(() => {
          setProgress((prev) => Math.min(prev + 5, 90));
        }, 1500);

        const result = await analysisAPI.executeEDA(sessionId);

        clearInterval(progressInterval);
        clearTimeout(timeout);
        setProgress(100);

        // Pass result to parent component
        props.onComplete(result);
      } catch (err: any) {
        clearTimeout(timeout);

        if (err.message.includes('Failed to fetch')) {
          setError('ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼: ã‚µãƒ¼ãƒãƒ¼ã«æ¥ç¶šã§ãã¾ã›ã‚“');
        } else {
          setError(err.message);
        }
      }
    };

    executeEDA();
  }, [sessionId]);

  if (error) {
    return (
      <div className="error-container">
        <h3>âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ</h3>
        <p>{error}</p>
        <button onClick={() => window.location.reload()}>ãƒªãƒˆãƒ©ã‚¤</button>
      </div>
    );
  }

  return (
    <div className="eda-loader">
      <div className="spinner"></div>
      <h3>ğŸ” EDAåˆ†æã‚’å®Ÿè¡Œä¸­...</h3>
      <div className="progress-bar">
        <div className="progress-fill" style={{ width: `${progress}%` }}></div>
      </div>
      <p>{progress}% å®Œäº† (æ¨å®šæ®‹ã‚Šæ™‚é–“: {Math.max(0, estimatedTime - (progress / 100 * estimatedTime)).toFixed(0)}ç§’)</p>
      <ul className="task-list">
        <li className={progress > 30 ? 'completed' : 'pending'}>âœ“ Sweetvizåˆ†æ</li>
        <li className={progress > 60 ? 'completed' : 'pending'}>âœ“ çµ±è¨ˆã‚µãƒãƒªãƒ¼æŠ½å‡º</li>
        <li className={progress > 90 ? 'completed' : 'pending'}>âœ“ LangChainæ´å¯Ÿç”Ÿæˆ</li>
      </ul>
    </div>
  );
};
```

**Error Boundary**:
```typescript
// frontend/src/components/ErrorBoundary.tsx

import React, { Component, ErrorInfo, ReactNode } from 'react';

interface Props {
  children: ReactNode;
}

interface State {
  hasError: boolean;
  error: Error | null;
}

export class ErrorBoundary extends Component<Props, State> {
  public state: State = {
    hasError: false,
    error: null
  };

  public static getDerivedStateFromError(error: Error): State {
    return { hasError: true, error };
  }

  public componentDidCatch(error: Error, errorInfo: ErrorInfo) {
    console.error('Error boundary caught:', error, errorInfo);
  }

  public render() {
    if (this.state.hasError) {
      return (
        <div className="error-boundary">
          <h2>ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ</h2>
          <details>
            <summary>è©³ç´°æƒ…å ±</summary>
            <pre>{this.state.error?.toString()}</pre>
          </details>
          <button onClick={() => window.location.reload()}>ãƒšãƒ¼ã‚¸ã‚’å†èª­ã¿è¾¼ã¿</button>
        </div>
      );
    }

    return this.props.children;
  }
}
```

**Testing Requirements**:
- Unit test: Error boundary catches errors
- Unit test: Timeout triggers after 60 seconds
- Integration test: Network failure displays proper message
- UX test: Loading indicators visible during execution

**Dependencies**: None (pure React)

---

**Sprint 3 Definition of Done**:
- [ ] All 3 user stories completed and tested
- [ ] Frontend components integrated into main dashboard
- [ ] End-to-end user flow tested: Upload â†’ Mode Select â†’ EDA Execute â†’ View Results
- [ ] Error states handled gracefully
- [ ] UI/UX reviewed and approved
- [ ] Performance validated (<30s EDA execution, responsive UI)

---

## âš ï¸ Phase 2: åŸå› ç‰¹å®šæ©Ÿèƒ½ - NOT TO BE STARTED YET

**Important**: Phase 2ï¼ˆè¦å› è§£æãƒ»åŸå› ç‰¹å®šæ©Ÿèƒ½ï¼‰ã¯Phase 1å®Œäº†å¾Œã«ã®ã¿ç€æ‰‹ã—ã¦ãã ã•ã„ã€‚

Phase 1ã®å®Œäº†æ¡ä»¶:
- âœ… å…¨3ã‚¹ãƒ—ãƒªãƒ³ãƒˆå®Œäº†ï¼ˆ18ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ï¼‰
- âœ… çµ±åˆãƒ†ã‚¹ãƒˆåˆæ ¼
- âœ… Productionç’°å¢ƒãƒ‡ãƒ—ãƒ­ã‚¤æˆåŠŸ
- âœ… ãƒ¦ãƒ¼ã‚¶ãƒ¼å—å…¥ãƒ†ã‚¹ãƒˆå®Œäº†

**Phase 2 Preview** (å®Ÿè£…ã¯ Phase 1 å®Œäº†å¾Œ):
- Sprint 4-6: è¦å› è§£æAI Agentçµ±åˆ
- Why-Whyåˆ†æã€ãƒ•ã‚£ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ³å›³ã€æ•£å¸ƒå›³åˆ†æ
- LangChain Agent with Tools (pandas analysis, statistical tests)
- å› æœæ¨è«–ãƒ©ã‚¤ãƒ–ãƒ©ãƒªçµ±åˆ

---

## Phase 2: Infrastructure (Weeks 7-12)

**Goal**: Scale system with persistent storage, distributed caching, and background task processing

### Sprint 4: Persistent Database (Week 7-8)

**Sprint Goal**: Migrate from session-based file storage to PostgreSQL with knowledge base foundation

#### User Stories

**US4.1 - PostgreSQL Database Setup** (Story Points: 5)
```
As a DevOps engineer
I want PostgreSQL database for persistent storage
So that data survives server restarts
```

**Acceptance Criteria**:
- âœ… PostgreSQL 14+ installed and configured
- âœ… Database schema created with migrations
- âœ… Connection pooling configured (pgbouncer or SQLAlchemy pool)
- âœ… Backup and restore procedures documented
- âœ… Automated backup schedule (daily, 30-day retention)

**Implementation Tasks**:
```python
# Task 4.1.1: Install PostgreSQL
# sudo apt-get install postgresql-14

# Task 4.1.2: Create database and user
# CREATE DATABASE qstorm_db;
# CREATE USER qstorm_user WITH PASSWORD 'secure_password';
# GRANT ALL PRIVILEGES ON DATABASE qstorm_db TO qstorm_user;

# Task 4.1.3: Add SQLAlchemy to app_improved.py
from flask_sqlalchemy import SQLAlchemy

app.config['SQLALCHEMY_DATABASE_URI'] = os.environ.get('DATABASE_URL')
app.config['SQLALCHEMY_TRACK_MODIFICATIONS'] = False
app.config['SQLALCHEMY_ENGINE_OPTIONS'] = {
    'pool_size': 10,
    'pool_recycle': 3600,
    'pool_pre_ping': True
}
db = SQLAlchemy(app)

# Task 4.1.4: Create migration framework (Alembic)
# alembic init migrations
# alembic revision --autogenerate -m "Initial schema"
# alembic upgrade head

# Task 4.1.5: Configure automated backups
# pg_dump qstorm_db > backup_$(date +%Y%m%d).sql
```

**Testing Requirements**:
- Connection test: App connects to PostgreSQL successfully
- Migration test: Schema migrations run without errors
- Backup test: Backup/restore procedure validated
- Failover test: App handles database unavailability gracefully

**Dependencies**: None

---

**US4.2 - Database Schema Implementation** (Story Points: 8)
```
As a developer
I want database models for sessions, analyses, and knowledge base
So that I can persist application data
```

**Acceptance Criteria**:
- âœ… `users` table (id, username, password_hash, role, created_at)
- âœ… `sessions` table (session_id, user_id, filename, upload_time, metadata JSON)
- âœ… `analysis_results` table (id, session_id, analysis_type, config JSON, result JSON, created_at)
- âœ… `knowledge_base` table (id, category, problem_type, insight, effectiveness_score, usage_count)
- âœ… Indexes on frequently queried columns
- âœ… Foreign key constraints for referential integrity

**Implementation Tasks**:
```python
# app_improved.py - Database models

from datetime import datetime

class User(db.Model):
    """User authentication and authorization"""
    __tablename__ = 'users'

    id = db.Column(db.Integer, primary_key=True)
    username = db.Column(db.String(80), unique=True, nullable=False)
    password_hash = db.Column(db.String(255), nullable=False)
    role = db.Column(db.String(20), default='analyst')  # admin, analyst, viewer
    created_at = db.Column(db.DateTime, default=datetime.utcnow)

    sessions = db.relationship('Session', backref='user', lazy=True)

class Session(db.Model):
    """Upload session tracking"""
    __tablename__ = 'sessions'

    session_id = db.Column(db.String(20), primary_key=True)
    user_id = db.Column(db.Integer, db.ForeignKey('users.id'), nullable=False)
    filename = db.Column(db.String(255), nullable=False)
    upload_time = db.Column(db.DateTime, default=datetime.utcnow)
    file_size = db.Column(db.Integer)
    rows = db.Column(db.Integer)
    columns = db.Column(db.Integer)
    metadata = db.Column(db.JSON)  # Store date_range, stores, etc.

    analysis_results = db.relationship('AnalysisResult', backref='session', lazy=True)

class AnalysisResult(db.Model):
    """Persistent analysis results"""
    __tablename__ = 'analysis_results'

    id = db.Column(db.Integer, primary_key=True)
    session_id = db.Column(db.String(20), db.ForeignKey('sessions.session_id'), nullable=False)
    analysis_type = db.Column(db.String(50), nullable=False)  # pareto, histogram, timeseries, etc.
    config = db.Column(db.JSON)  # ParetoConfig, filters, etc.
    result = db.Column(db.JSON)  # Complete ParetoResult as JSON
    execution_time_ms = db.Column(db.Integer)
    created_at = db.Column(db.DateTime, default=datetime.utcnow)

    __table_args__ = (
        db.Index('idx_session_type', 'session_id', 'analysis_type'),
    )

class KnowledgeBase(db.Model):
    """Self-evolving knowledge system (QC Story insights)"""
    __tablename__ = 'knowledge_base'

    id = db.Column(db.Integer, primary_key=True)
    category = db.Column(db.String(50), nullable=False)  # quality, cost, leadtime, other
    problem_type = db.Column(db.String(100))  # High inventory, low sales, etc.
    insight = db.Column(db.Text, nullable=False)  # Natural language insight
    effectiveness_score = db.Column(db.Float, default=0.0)  # 0-1 score
    usage_count = db.Column(db.Integer, default=0)
    last_used = db.Column(db.DateTime)
    created_at = db.Column(db.DateTime, default=datetime.utcnow)
    updated_at = db.Column(db.DateTime, onupdate=datetime.utcnow)

    __table_args__ = (
        db.Index('idx_category_score', 'category', 'effectiveness_score'),
    )

# Task 4.2.1: Define models above
# Task 4.2.2: Create migration
# Task 4.2.3: Apply migration to database
# Task 4.2.4: Implement CRUD operations for each model
# Task 4.2.5: Add data access layer functions
```

**Testing Requirements**:
- Model test: All models create/read/update/delete successfully
- Relationship test: Foreign keys enforce referential integrity
- Index test: Queries use indexes (EXPLAIN ANALYZE)
- Migration test: Schema can be upgraded and downgraded

**Dependencies**: US4.1 (PostgreSQL Setup)

---

**US4.3 - Dual-Write Migration Strategy** (Story Points: 5)
```
As a system
I want to write to both file storage and database during migration
So that I can safely transition without data loss
```

**Acceptance Criteria**:
- âœ… File upload writes to both `uploads/` directory and `sessions` table
- âœ… Analysis results written to both in-memory cache and `analysis_results` table
- âœ… Read operations check database first, fallback to file storage
- âœ… Migration complete flag to switch to database-only mode
- âœ… File cleanup job to archive old session files

**Implementation Tasks**:
```python
# app_improved.py - Dual-write pattern

@app.route('/api/v1/upload', methods=['POST'])
@jwt_required()
@limiter.limit("10 per hour")
def upload_file():
    """Upload file with dual-write to disk and database"""
    user_id = get_jwt_identity()

    # Existing file storage logic (app_improved.py:500-550)
    session_id = generate_session_id()
    filepath = save_uploaded_file(file, session_id)

    # NEW: Write to database (dual-write)
    session = Session(
        session_id=session_id,
        user_id=user_id,
        filename=file.filename,
        file_size=os.path.getsize(filepath),
        rows=len(df),
        columns=len(df.columns),
        metadata={
            'date_range': {'start': min_date, 'end': max_date},
            'stores': df['shop'].unique().tolist()
        }
    )
    db.session.add(session)
    db.session.commit()

    # Return session_id as before

# Task 4.3.1: Implement dual-write for uploads
# Task 4.3.2: Implement dual-write for analysis results
# Task 4.3.3: Add database-first read logic with file fallback
# Task 4.3.4: Create migration_complete flag
# Task 4.3.5: Implement file cleanup job (move old files to archive/)
```

**Testing Requirements**:
- Dual-write test: Both storage mechanisms updated
- Fallback test: Database unavailable â†’ file storage works
- Migration test: Existing sessions accessible after migration
- Cleanup test: Old files moved to archive correctly

**Dependencies**: US4.2 (Database Schema)

---

**US4.4 - Knowledge Base Foundation** (Story Points: 3)
```
As an AI agent
I want to store and retrieve QC Story insights from knowledge base
So that the system learns from previous analyses
```

**Acceptance Criteria**:
- âœ… CRUD operations for `knowledge_base` table
- âœ… Insight search by category and problem type
- âœ… Effectiveness score update mechanism
- âœ… Top insights retrieval (by effectiveness score)
- âœ… API endpoint `/api/v2/knowledge/insights` for retrieval

**Implementation Tasks**:
```python
# Task 4.4.1: Implement knowledge base CRUD operations
def add_insight(category, problem_type, insight_text):
    """Add new insight to knowledge base"""
    insight = KnowledgeBase(
        category=category,
        problem_type=problem_type,
        insight=insight_text,
        effectiveness_score=0.5  # Initial neutral score
    )
    db.session.add(insight)
    db.session.commit()
    return insight.id

def get_insights(category=None, min_score=0.7, limit=10):
    """Retrieve top insights by effectiveness score"""
    query = KnowledgeBase.query
    if category:
        query = query.filter_by(category=category)
    query = query.filter(KnowledgeBase.effectiveness_score >= min_score)
    query = query.order_by(KnowledgeBase.effectiveness_score.desc())
    return query.limit(limit).all()

def update_effectiveness_score(insight_id, feedback_score):
    """Update effectiveness score based on user feedback"""
    insight = KnowledgeBase.query.get(insight_id)
    # Exponential moving average: new_score = 0.8 * old_score + 0.2 * feedback
    insight.effectiveness_score = 0.8 * insight.effectiveness_score + 0.2 * feedback_score
    insight.usage_count += 1
    insight.last_used = datetime.utcnow()
    db.session.commit()

# Task 4.4.2: Create API endpoint for insights
@app.route('/api/v2/knowledge/insights', methods=['GET'])
@jwt_required()
def get_knowledge_insights():
    """Retrieve insights from knowledge base"""
    category = request.args.get('category')
    insights = get_insights(category=category)
    return jsonify({
        'insights': [
            {
                'id': i.id,
                'category': i.category,
                'problem_type': i.problem_type,
                'insight': i.insight,
                'effectiveness_score': i.effectiveness_score,
                'usage_count': i.usage_count
            }
            for i in insights
        ]
    })

# Task 4.4.3: Seed knowledge base with initial QC Story insights
# Task 4.4.4: Implement feedback endpoint for score updates
```

**Testing Requirements**:
- CRUD test: All knowledge base operations work
- Search test: Insights filtered correctly by category
- Score test: Effectiveness score updates correctly
- API test: Endpoint returns valid JSON

**Dependencies**: US4.2 (Database Schema)

---

#### Sprint 4 Deliverables

- âœ… PostgreSQL database operational with complete schema
- âœ… Dual-write migration strategy implemented
- âœ… Knowledge base foundation ready for AI agent integration
- âœ… Database backup and restore procedures validated
- âœ… All data persists across server restarts

#### Sprint 4 Definition of Done

- [ ] All database migrations applied successfully
- [ ] Database performance tested (>1000 TPS)
- [ ] Backup/restore procedure validated
- [ ] Data retention policy documented
- [ ] Database monitoring configured (pg_stat_statements)
- [ ] No data loss during migration from file storage

---

### Sprint 5: Caching & Background Tasks (Week 9-10)

**Sprint Goal**: Implement Redis for distributed caching and Celery for asynchronous task processing

#### User Stories

**US5.1 - Redis Caching Infrastructure** (Story Points: 5)
```
As a system
I want Redis for distributed caching and session management
So that analysis results are shared across server instances
```

**Acceptance Criteria**:
- âœ… Redis 6+ installed and configured
- âœ… Session storage migrated from file system to Redis
- âœ… Analysis result caching in Redis with TTL
- âœ… Cache invalidation on new uploads
- âœ… Redis monitoring and health checks

**Implementation Tasks**:
```python
# Task 5.1.1: Install Redis
# sudo apt-get install redis-server

# Task 5.1.2: Configure Redis connection
import redis
from flask_caching import Cache

cache = Cache(app, config={
    'CACHE_TYPE': 'redis',
    'CACHE_REDIS_URL': os.environ.get('REDIS_URL', 'redis://localhost:6379/0'),
    'CACHE_DEFAULT_TIMEOUT': 3600  # 1 hour
})

# Task 5.1.3: Migrate session storage to Redis
from flask_session import Session

app.config['SESSION_TYPE'] = 'redis'
app.config['SESSION_REDIS'] = redis.from_url(os.environ.get('REDIS_URL'))
Session(app)

# Task 5.1.4: Implement cache decorators for analysis functions
@cache.memoize(timeout=3600)
def cached_pareto_analysis(data_hash, config_hash):
    """Cache Pareto analysis results for 1 hour"""
    # Existing ParetoAnalysisEngine.analyze() logic
    pass

# Task 5.1.5: Add cache invalidation on upload
@app.route('/api/v1/upload', methods=['POST'])
def upload_file():
    # Existing upload logic
    # Clear all cached analyses for this user
    cache.delete_memoized(cached_pareto_analysis)

# Task 5.1.6: Implement Redis health check
@app.route('/api/v2/health', methods=['GET'])
def health_check():
    redis_status = "healthy" if cache.cache._client.ping() else "unhealthy"
    return jsonify({'redis': redis_status})
```

**Testing Requirements**:
- Connection test: App connects to Redis successfully
- Cache test: Analysis results cached and retrieved correctly
- Invalidation test: Cache cleared on new uploads
- Failover test: App handles Redis unavailability (fallback to database)
- Performance test: Cache hit rate >80% for repeated analyses

**Dependencies**: None

---

**US5.2 - Celery Task Queue Setup** (Story Points: 8)
```
As a system
I want Celery for background task processing
So that long-running analyses don't block API responses
```

**Acceptance Criteria**:
- âœ… Celery 5+ configured with Redis broker
- âœ… Task result backend configured (Redis)
- âœ… Worker process running and monitored
- âœ… Task retry logic with exponential backoff
- âœ… Task monitoring dashboard (Flower)

**Implementation Tasks**:
```python
# Task 5.2.1: Install Celery
# pip install celery[redis] flower

# Task 5.2.2: Configure Celery
from celery import Celery

def make_celery(app):
    celery = Celery(
        app.import_name,
        broker=os.environ.get('REDIS_URL', 'redis://localhost:6379/0'),
        backend=os.environ.get('REDIS_URL', 'redis://localhost:6379/0')
    )
    celery.conf.update(app.config)
    return celery

celery = make_celery(app)

# Task 5.2.3: Define background tasks
@celery.task(bind=True, max_retries=3)
def async_pareto_analysis(self, session_id, config):
    """Background Pareto analysis task"""
    try:
        # Load session data
        df = load_session_data(session_id)

        # Perform analysis
        engine = ParetoAnalysisEngine()
        result = engine.analyze(df, config)

        # Save to database
        analysis = AnalysisResult(
            session_id=session_id,
            analysis_type='pareto',
            config=config.to_dict(),
            result=result.to_dict(),
            execution_time_ms=result.metadata['execution_time_ms']
        )
        db.session.add(analysis)
        db.session.commit()

        return {'status': 'success', 'analysis_id': analysis.id}

    except Exception as exc:
        # Retry with exponential backoff
        raise self.retry(exc=exc, countdown=2 ** self.request.retries)

# Task 5.2.4: Create API endpoint for async analysis
@app.route('/api/v2/analysis/pareto/async', methods=['POST'])
@jwt_required()
def async_pareto():
    """Submit Pareto analysis task to background queue"""
    data = request.json
    task = async_pareto_analysis.delay(data['session_id'], data['config'])
    return jsonify({
        'status': 'processing',
        'task_id': task.id,
        'poll_url': f'/api/v2/tasks/{task.id}'
    }), 202

# Task 5.2.5: Create task status polling endpoint
@app.route('/api/v2/tasks/<task_id>', methods=['GET'])
@jwt_required()
def get_task_status(task_id):
    """Poll task status"""
    task = async_pareto_analysis.AsyncResult(task_id)
    if task.state == 'PENDING':
        response = {'state': task.state, 'status': 'Task is waiting to be executed'}
    elif task.state == 'SUCCESS':
        response = {'state': task.state, 'result': task.result}
    elif task.state == 'FAILURE':
        response = {'state': task.state, 'error': str(task.info)}
    else:
        response = {'state': task.state, 'status': str(task.info)}
    return jsonify(response)

# Task 5.2.6: Start Celery worker
# celery -A app_improved.celery worker --loglevel=info

# Task 5.2.7: Start Flower monitoring
# celery -A app_improved.celery flower --port=5555
```

**Testing Requirements**:
- Task submission test: Tasks queued successfully
- Task execution test: Workers process tasks correctly
- Retry test: Failed tasks retry with backoff
- Monitoring test: Flower dashboard accessible
- Load test: 100 concurrent tasks processed

**Dependencies**: US5.1 (Redis Infrastructure)

---

**US5.3 - Background Knowledge Base Updates** (Story Points: 5)
```
As a system
I want to automatically update knowledge base effectiveness scores
So that the system evolves based on usage patterns
```

**Acceptance Criteria**:
- âœ… Scheduled task runs daily to recalculate effectiveness scores
- âœ… Scores updated based on usage frequency and recency
- âœ… Low-scoring insights archived (score <0.3 for >30 days)
- âœ… New insights auto-generated from top Pareto analyses
- âœ… Task logs results for debugging

**Implementation Tasks**:
```python
# Task 5.3.1: Define scheduled task (Celery Beat)
from celery.schedules import crontab

celery.conf.beat_schedule = {
    'update-knowledge-scores-daily': {
        'task': 'app_improved.update_knowledge_scores',
        'schedule': crontab(hour=2, minute=0),  # 2 AM daily
    },
}

# Task 5.3.2: Implement score update logic
@celery.task
def update_knowledge_scores():
    """Recalculate effectiveness scores for all insights"""
    insights = KnowledgeBase.query.all()

    for insight in insights:
        # Decay score for unused insights
        days_since_use = (datetime.utcnow() - insight.last_used).days if insight.last_used else 999
        decay_factor = 0.95 ** (days_since_use / 7)  # 5% decay per week

        # Boost score for frequently used insights
        usage_boost = min(0.2, insight.usage_count / 100)  # Max 0.2 boost

        # Update score
        new_score = insight.effectiveness_score * decay_factor + usage_boost
        insight.effectiveness_score = max(0.0, min(1.0, new_score))  # Clamp to [0, 1]

        # Archive low-scoring old insights
        if insight.effectiveness_score < 0.3 and days_since_use > 30:
            insight.archived = True

    db.session.commit()
    return f"Updated {len(insights)} insights"

# Task 5.3.3: Implement auto-insight generation from analyses
@celery.task
def generate_insights_from_analyses():
    """Extract insights from recent Pareto analyses"""
    # Get analyses from last 7 days
    recent = AnalysisResult.query.filter(
        AnalysisResult.created_at >= datetime.utcnow() - timedelta(days=7)
    ).all()

    for analysis in recent:
        # Extract vital_few categories
        vital_few = analysis.result['vital_few']
        if len(vital_few) <= 3:  # Strong Pareto concentration
            insight_text = f"{', '.join(vital_few)} ãŒå…¨ä½“ã®80%ã‚’å ã‚ã¦ã„ã¾ã™"
            add_insight(
                category='pareto_concentration',
                problem_type='vital_few_identified',
                insight_text=insight_text
            )

    return f"Generated insights from {len(recent)} analyses"

# Task 5.3.4: Start Celery Beat scheduler
# celery -A app_improved.celery beat --loglevel=info
```

**Testing Requirements**:
- Schedule test: Task runs at configured time
- Score update test: Effectiveness scores change correctly
- Archive test: Low-scoring insights archived
- Insight generation test: New insights created from analyses

**Dependencies**: US4.4 (Knowledge Base Foundation), US5.2 (Celery Setup)

---

#### Sprint 5 Deliverables

- âœ… Redis caching operational with >80% hit rate
- âœ… Celery task queue processing background analyses
- âœ… Knowledge base auto-updates running on schedule
- âœ… Flower monitoring dashboard accessible
- âœ… System handles 50+ concurrent users without degradation

#### Sprint 5 Definition of Done

- [ ] Redis and Celery stable under load (>99% uptime)
- [ ] Cache hit rate monitored and optimized
- [ ] Task queue processing <1s latency
- [ ] Background jobs logged and monitored
- [ ] Failover procedures tested (Redis/Celery restart)
- [ ] Performance improvement validated (>50% faster for repeated analyses)

---

### Sprint 6: Enhanced Authentication (Week 11-12)

**Sprint Goal**: Production-grade authentication with user management and audit logging

#### User Stories

**US6.1 - User Management Interface** (Story Points: 5)
```
As an administrator
I want to manage users (create, update, delete, reset passwords)
So that I can control system access
```

**Acceptance Criteria**:
- âœ… Admin dashboard for user management
- âœ… Create user endpoint with email validation
- âœ… Update user role endpoint (admin only)
- âœ… Reset password endpoint with email verification
- âœ… Delete user endpoint with cascading session cleanup
- âœ… List users with pagination and search

**Implementation Tasks** (Omitted for brevity - follow similar pattern to Sprint 3)

**Dependencies**: Sprint 3 US3.1-US3.2

---

**US6.2 - Audit Logging System** (Story Points: 3)
```
As a security officer
I want audit logs for all sensitive operations
So that I can track system access and changes
```

**Acceptance Criteria**:
- âœ… Audit log table created (user, action, timestamp, details)
- âœ… All authentication events logged (login, logout, failed attempts)
- âœ… All data modifications logged (upload, delete, update)
- âœ… Audit log query API for administrators
- âœ… Log retention policy enforced (90 days)

**Implementation Tasks** (Omitted for brevity)

**Dependencies**: Sprint 4 US4.2 (Database Schema)

---

**US6.3 - Session Management** (Story Points: 3)
```
As a user
I want to manage my active sessions
So that I can log out from all devices
```

**Acceptance Criteria**:
- âœ… Active session list endpoint
- âœ… Logout from specific session
- âœ… Logout from all sessions (invalidate all tokens)
- âœ… Session timeout after 24 hours of inactivity
- âœ… Session extended on user activity

**Implementation Tasks** (Omitted for brevity)

**Dependencies**: Sprint 3 US3.1, Sprint 5 US5.1 (Redis for session storage)

---

#### Sprint 6 Deliverables

- âœ… Complete user management system
- âœ… Comprehensive audit logging
- âœ… Session management with timeout
- âœ… **PRODUCTION v2.0 READY** (with persistence and auth)

---

## Phase 3: Intelligence (Weeks 13-18)

**Goal**: Integrate AI agents for QC Story methodology with intelligent insights and optimization

### Sprint 7: AI Agent 1 - ç¾çŠ¶æŠŠæ¡ (Week 13-14)

**Sprint Goal**: Implement Current State Analysis with AutoGluon feature importance

#### User Stories

**US7.1 - AutoGluon Integration** (Story Points: 8)

(Details omitted for brevity - follows spec Section 8.1.2)

---

### Sprint 8: AI Agent 2 - åŸå› ç‰¹å®š (Week 15-16)

**Sprint Goal**: Implement Root Cause Analysis with causal inference

#### User Stories

**US8.1 - Fishbone Diagram Generation** (Story Points: 5)
**US8.2 - Logistic Regression Analysis** (Story Points: 8)
**US8.3 - Causal Inference** (Story Points: 8)

(Details omitted for brevity)

---

### Sprint 9: AI Agent 3 - åŠ¹æœäºˆæ¸¬ãƒ»æœ€é©åŒ– (Week 17-18)

**Sprint Goal**: Implement Effect Prediction and Optimization with PyMC and Gurobi

#### User Stories

**US9.1 - PyMC MCMC Analysis** (Story Points: 8)
**US9.2 - Gurobi Optimization** (Story Points: 8)
**US9.3 - ROI Analysis** (Story Points: 5)

(Details omitted for brevity)

---

## Cross-Cutting Concerns

### Testing Strategy

**Test Pyramid**:
- **Unit Tests** (70%): All business logic, data processing, utilities
- **Integration Tests** (20%): API endpoints, database operations, external services
- **E2E Tests** (10%): Complete user workflows

**Test Coverage Targets**:
- Phase 1: >70%
- Phase 2: >80%
- Phase 3: >85%

**Test Automation**:
```bash
# Run all tests
pytest tests/ --cov=app_improved --cov-report=html

# Run specific test suite
pytest tests/test_pareto_system.py -v

# Run integration tests only
pytest tests/integration/ -m integration

# Run with coverage threshold enforcement
pytest --cov=app_improved --cov-fail-under=80
```

---

### DevOps & Deployment

**CI/CD Pipeline**:

1. **Commit Stage** (Triggered on every commit):
   - Linting (flake8, mypy)
   - Unit tests
   - Security scan (Bandit)

2. **Build Stage** (Triggered on PR to main):
   - Integration tests
   - Build Docker image
   - Push to staging registry

3. **Deploy Stage** (Triggered on merge to main):
   - Deploy to staging
   - Smoke tests
   - Manual approval
   - Blue-green deployment to production

**Production Deployment Checklist**:

- [ ] Database migrations tested and documented
- [ ] Backup/rollback procedures validated
- [ ] Health check endpoints responding
- [ ] Monitoring alerts configured
- [ ] Rate limiting tested under load
- [ ] Security scan passed (no critical vulnerabilities)
- [ ] Load testing completed (target: 50 concurrent users, <3s response)
- [ ] Documentation updated
- [ ] Incident response runbook reviewed
- [ ] Stakeholder sign-off obtained

---

### Monitoring & Observability

**Application Metrics**:
- Request rate, error rate, duration (RED metrics)
- Cache hit rate (Redis)
- Task queue length (Celery)
- Database connection pool utilization

**Infrastructure Metrics**:
- CPU, memory, disk usage
- Network I/O
- PostgreSQL query performance (slow query log)
- Redis memory usage

**Alerting Rules**:
- Error rate >5% for 5 minutes â†’ Page on-call engineer
- Response time >5s for 10 minutes â†’ Warning
- Memory usage >90% â†’ Critical alert
- Database connections >90% of pool â†’ Warning
- Celery queue depth >100 tasks â†’ Warning

**Logging**:
```python
import logging
import structlog

# Structured logging for production
structlog.configure(
    processors=[
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.processors.StackInfoRenderer(),
        structlog.processors.format_exc_info,
        structlog.processors.JSONRenderer()
    ],
    logger_factory=structlog.PrintLoggerFactory(),
)

logger = structlog.get_logger()

# Usage
logger.info("pareto_analysis_completed",
            session_id=session_id,
            categories=len(result.categories),
            execution_time_ms=result.metadata['execution_time_ms'])
```

---

## Risk Management

### Technical Risks

| Risk | Probability | Impact | Mitigation |
|------|------------|--------|------------|
| **Single-file architecture becomes unmaintainable** | High | High | Aggressive use of nested classes, comprehensive inline documentation |
| **psutil unavailable on production** | Medium | Medium | Graceful degradation to static limits, include in requirements.txt |
| **OpenAI API rate limits** | Medium | High | Implement caching, rate limiting, fallback to template-based explanations |
| **Database migration data loss** | Low | Critical | Comprehensive testing, dual-write strategy, automated backups |
| **Celery task queue overflow** | Medium | Medium | Task TTL, queue depth monitoring, worker auto-scaling |

### Schedule Risks

| Risk | Probability | Impact | Mitigation |
|------|------------|--------|------------|
| **Sprint 1-2 delays block Phase 1** | Medium | High | Critical path protection, daily standups, early escalation |
| **AutoGluon integration complexity** | High | Medium | Spike in Sprint 6 to validate approach, fallback to simpler ML |
| **PyMC/Gurobi licensing issues** | Low | High | Early procurement, open-source fallback (PuLP) |
| **Team availability issues** | Medium | Medium | Cross-training, documentation, pair programming |

---

## Success Criteria Summary

### Phase 1 Success Criteria (Production v1.0)
- âœ… All backend API endpoints implemented and tested
- âœ… Frontend fully integrated with working visualizations
- âœ… JWT authentication protecting all sensitive endpoints
- âœ… Security audit passed (no critical/high vulnerabilities)
- âœ… Load testing: 10 concurrent users, <5s response time
- âœ… Test coverage >70%
- âœ… Zero critical bugs in production

### Phase 2 Success Criteria (Production v2.0)
- âœ… PostgreSQL operational with all data persisted
- âœ… Redis caching >80% hit rate
- âœ… Celery processing background tasks successfully
- âœ… User management and audit logging functional
- âœ… Load testing: 50 concurrent users, <3s response time
- âœ… Test coverage >80%
- âœ… 99.5% uptime over 30 days

### Phase 3 Success Criteria (Production v3.0)
- âœ… All 3 AI agents (ç¾çŠ¶æŠŠæ¡, åŸå› ç‰¹å®š, åŠ¹æœäºˆæ¸¬) operational
- âœ… Knowledge base self-evolving with >100 insights
- âœ… AI insights accuracy >80% (validated against historical data)
- âœ… User satisfaction score >90%
- âœ… Test coverage >85%
- âœ… 99.9% uptime over 30 days

---

## Appendices

### Appendix A: Technology Stack Summary

| Layer | Technology | Version | Purpose |
|-------|-----------|---------|---------|
| **Backend** | Python | 3.11 | Core runtime |
| | Flask | 2.3 | Web framework |
| | SQLAlchemy | 2.0 | ORM |
| | Celery | 5.3 | Task queue |
| | Redis | 6.2 | Caching & broker |
| | PostgreSQL | 14 | Database |
| **Frontend** | React | 18 | UI framework |
| | TypeScript | 5.0 | Type safety |
| | Plotly.js | 2.x | Visualization |
| | Ant Design | 5.x | Component library |
| | React Query | 4.x | Data fetching |
| **AI/ML** | AutoGluon | 0.8 | AutoML |
| | LangChain | 0.0.200 | LLM integration |
| | PyMC | 5.x | MCMC |
| | Gurobi | 10.x | Optimization |
| **DevOps** | Docker | 24.x | Containerization |
| | Nginx | 1.24 | Reverse proxy |
| | Gunicorn | 21.x | WSGI server |

### Appendix B: Sprint Velocity Assumptions

**Story Point Estimation**:
- 1 point = 1-2 hours of development
- 3 points = 4-6 hours (half day)
- 5 points = 1-2 days
- 8 points = 3-5 days
- 13 points = 1 week (split into smaller stories)

**Team Capacity** (per 2-week sprint):
- Backend Developer: 30-40 points
- Frontend Developer: 30-40 points
- DevOps Engineer: 20-30 points (shared responsibility)
- QA Engineer: 20-30 points (testing all sprints)

**Total Sprint Capacity**: 100-140 points across team

### Appendix C: Definition of Ready (DoR)

Before a user story enters a sprint:
- [ ] Acceptance criteria clearly defined
- [ ] Dependencies identified and resolved
- [ ] Design mockups available (if UI work)
- [ ] API contract agreed upon (if backend work)
- [ ] Test scenarios documented
- [ ] Story points estimated by team
- [ ] No blockers or external dependencies

### Appendix D: Rollback Procedures

**Database Rollback**:
```bash
# Rollback last migration
alembic downgrade -1

# Rollback to specific version
alembic downgrade <revision_id>

# Restore from backup
pg_restore -d qstorm_db backup_20250115.sql
```

**Application Rollback (Blue-Green)**:
```bash
# Switch traffic back to blue (previous version)
# Update load balancer configuration
# Verify health checks pass on blue environment
# Monitor error rates for 10 minutes
# If stable, decommission green environment
```

**Cache Invalidation**:
```bash
# Clear all Redis cache after rollback
redis-cli FLUSHDB
```

### Appendix E: LangChainçµ±åˆã‚¬ã‚¤ãƒ‰

ã“ã®Appendixã¯ã€Q-Storm Platformã«LangChain + OpenAI APIã‚’çµ±åˆã™ã‚‹ãŸã‚ã®ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã€ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã€ã‚³ã‚¹ãƒˆç®¡ç†ã‚¬ã‚¤ãƒ‰ã§ã™ã€‚

#### E.1 ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

**Step 1: OpenAI APIã‚­ãƒ¼ã®å–å¾—**

1. OpenAI Platform (https://platform.openai.com/) ã«ã‚¢ã‚¯ã‚»ã‚¹
2. ã‚¢ã‚«ã‚¦ãƒ³ãƒˆä½œæˆã¾ãŸã¯ãƒ­ã‚°ã‚¤ãƒ³
3. "API keys" ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‹ã‚‰æ–°ã—ã„APIã‚­ãƒ¼ã‚’ç”Ÿæˆ
4. ç”Ÿæˆã•ã‚ŒãŸã‚­ãƒ¼ï¼ˆ`sk-proj-...`å½¢å¼ï¼‰ã‚’å®‰å…¨ã«ä¿å­˜

**Step 2: ä¾å­˜ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«**

```bash
# LangChain + OpenAI dependencies
pip install langchain==0.1.0 openai==1.12.0

# äº’æ›æ€§ç¢ºèª
python3 -c "import langchain; import openai; print('âœ… Dependencies installed')"
```

**Step 3: ç’°å¢ƒå¤‰æ•°ã®è¨­å®š**

**Linux/Mac**:
```bash
# ~/.bashrc ã¾ãŸã¯ ~/.zshrc ã«è¿½åŠ 
export OPENAI_API_KEY='sk-proj-xxxxxxxxxxxxxxxxxxxx'

# å³åº§ã«åæ˜ 
source ~/.bashrc  # ã¾ãŸã¯ source ~/.zshrc

# ç¢ºèª
echo $OPENAI_API_KEY | head -c 10  # æœ€åˆã®10æ–‡å­—ã®ã¿è¡¨ç¤º
```

**Windows (PowerShell)**:
```powershell
# ç¾åœ¨ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®ã¿
$env:OPENAI_API_KEY = "sk-proj-xxxxxxxxxxxxxxxxxxxx"

# æ°¸ç¶šçš„ã«è¨­å®š
[System.Environment]::SetEnvironmentVariable('OPENAI_API_KEY', 'sk-proj-xxxxxxxxxxxxxxxxxxxx', 'User')

# ç¢ºèª
$env:OPENAI_API_KEY.Substring(0, 10)
```

**Dockerç’°å¢ƒ**:
```bash
# .env ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆï¼ˆ.gitignoreã«è¿½åŠ å¿…é ˆï¼‰
echo "OPENAI_API_KEY=sk-proj-xxxxxxxxxxxxxxxxxxxx" > .env

# Docker Compose
version: '3.8'
services:
  app:
    env_file:
      - .env
    # ã¾ãŸã¯
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
```

**Step 4: å‹•ä½œç¢ºèªã‚¹ã‚¯ãƒªãƒ—ãƒˆ**

```python
#!/usr/bin/env python3
# test_langchain_setup.py

import os
import sys
from langchain.chat_models import ChatOpenAI
from langchain.schema import HumanMessage

def test_langchain_setup():
    """LangChainã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ç¢ºèª"""

    # Step 1: API Keyç¢ºèª
    print("ğŸ” Step 1: OpenAI API Keyç¢ºèª...")
    api_key = os.environ.get('OPENAI_API_KEY', '')

    if not api_key:
        print("âŒ OPENAI_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        print("   è¨­å®šæ–¹æ³•: export OPENAI_API_KEY='sk-proj-...'")
        return False

    if not api_key.startswith('sk-'):
        print("âŒ OPENAI_API_KEY ã®å½¢å¼ãŒä¸æ­£ã§ã™")
        print(f"   ç¾åœ¨ã®å€¤: {api_key[:10]}... (sk-ã§å§‹ã¾ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™)")
        return False

    print(f"âœ… API Keyæ¤œå‡º: {api_key[:7]}***{api_key[-4:]}")

    # Step 2: LangChainåˆæœŸåŒ–
    print("\nğŸ” Step 2: LangChain ChatOpenAIåˆæœŸåŒ–...")
    try:
        llm = ChatOpenAI(
            model_name="gpt-3.5-turbo",
            temperature=0.7,
            openai_api_key=api_key,
            request_timeout=30
        )
        print("âœ… ChatOpenAIåˆæœŸåŒ–æˆåŠŸ")
    except Exception as e:
        print(f"âŒ LangChainåˆæœŸåŒ–å¤±æ•—: {e}")
        return False

    # Step 3: APIå‘¼ã³å‡ºã—ãƒ†ã‚¹ãƒˆ
    print("\nğŸ” Step 3: OpenAI APIå‘¼ã³å‡ºã—ãƒ†ã‚¹ãƒˆ...")
    try:
        response = llm([HumanMessage(content="ã“ã‚“ã«ã¡ã¯ã€‚ç°¡æ½”ã«æŒ¨æ‹¶ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚")])
        print("âœ… LangChain APIå‘¼ã³å‡ºã—æˆåŠŸ")
        print(f"   å¿œç­”: {response.content}")
        return True
    except Exception as e:
        print(f"âŒ LangChain APIå‘¼ã³å‡ºã—å¤±æ•—: {e}")
        if "authentication" in str(e).lower():
            print("   â†’ APIã‚­ãƒ¼ãŒç„¡åŠ¹ã§ã™ã€‚OpenAI Platformã§ç¢ºèªã—ã¦ãã ã•ã„")
        elif "quota" in str(e).lower():
            print("   â†’ APIåˆ©ç”¨åˆ¶é™ã«é”ã—ã¦ã„ã¾ã™ã€‚èª²é‡‘è¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„")
        elif "timeout" in str(e).lower():
            print("   â†’ ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸã€‚ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šã‚’ç¢ºèªã—ã¦ãã ã•ã„")
        return False

if __name__ == '__main__':
    print("=" * 60)
    print("Q-Storm Platform - LangChainçµ±åˆã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ç¢ºèª")
    print("=" * 60)

    success = test_langchain_setup()

    print("\n" + "=" * 60)
    if success:
        print("âœ… ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å®Œäº†: LangChainçµ±åˆãŒæ­£å¸¸ã«å‹•ä½œã—ã¦ã„ã¾ã™")
        print("   æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—: python3 app_improved.py ã§ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’èµ·å‹•")
        sys.exit(0)
    else:
        print("âŒ ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å¤±æ•—: ä¸Šè¨˜ã®ã‚¨ãƒ©ãƒ¼ã‚’è§£æ±ºã—ã¦ãã ã•ã„")
        print("   ãƒ˜ãƒ«ãƒ—: Appendix E ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’å‚ç…§")
        sys.exit(1)
```

**å®Ÿè¡Œ**:
```bash
python3 test_langchain_setup.py
```

#### E.2 ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

**å•é¡Œ1: `OPENAI_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“`**

**åŸå› **: ç’°å¢ƒå¤‰æ•°ãŒæ­£ã—ãè¨­å®šã•ã‚Œã¦ã„ãªã„ã€ã¾ãŸã¯ã‚·ã‚§ãƒ«ã‚»ãƒƒã‚·ãƒ§ãƒ³ã§èª­ã¿è¾¼ã¾ã‚Œã¦ã„ãªã„

**è§£æ±ºç­–**:
```bash
# ç’°å¢ƒå¤‰æ•°ã‚’ç¢ºèª
env | grep OPENAI_API_KEY

# è¨­å®šã•ã‚Œã¦ã„ãªã„å ´åˆ
export OPENAI_API_KEY='sk-proj-...'

# æ°¸ç¶šåŒ–ï¼ˆ~/.bashrcã«è¿½åŠ ï¼‰
echo "export OPENAI_API_KEY='sk-proj-...'" >> ~/.bashrc
source ~/.bashrc
```

---

**å•é¡Œ2: `AuthenticationError: Incorrect API key provided`**

**åŸå› **: APIã‚­ãƒ¼ãŒç„¡åŠ¹ã€æœŸé™åˆ‡ã‚Œã€ã¾ãŸã¯èª¤ã£ã¦å…¥åŠ›ã•ã‚Œã¦ã„ã‚‹

**è§£æ±ºç­–**:
1. OpenAI Platform (https://platform.openai.com/api-keys) ã§APIã‚­ãƒ¼ã‚’ç¢ºèª
2. æ–°ã—ã„APIã‚­ãƒ¼ã‚’ç”Ÿæˆã—ã¦å†è¨­å®š
3. ã‚­ãƒ¼ã«ä½™åˆ†ãªã‚¹ãƒšãƒ¼ã‚¹ã‚„æ”¹è¡ŒãŒå«ã¾ã‚Œã¦ã„ãªã„ã‹ç¢ºèª

```bash
# ã‚­ãƒ¼ã®å‰å¾Œã®ç©ºç™½ã‚’å‰Šé™¤
export OPENAI_API_KEY=$(echo $OPENAI_API_KEY | xargs)

# å†ãƒ†ã‚¹ãƒˆ
python3 test_langchain_setup.py
```

---

**å•é¡Œ3: `RateLimitError: You exceeded your current quota`**

**åŸå› **: OpenAI APIã®ç„¡æ–™æ ã‚’ä½¿ã„æœãŸã—ãŸã€ã¾ãŸã¯èª²é‡‘è¨­å®šãŒã•ã‚Œã¦ã„ãªã„

**è§£æ±ºç­–**:
1. OpenAI Platform > Billing > Usage ã§APIä½¿ç”¨é‡ã‚’ç¢ºèª
2. èª²é‡‘æ–¹æ³•ã‚’è¿½åŠ : OpenAI Platform > Billing > Payment methods
3. ä½¿ç”¨åˆ¶é™ã‚’è¨­å®š: OpenAI Platform > Billing > Limits

**ä¸€æ™‚çš„å›é¿ç­–ï¼ˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ä½¿ç”¨ï¼‰**:
```python
# app_improved.py ã§ã¯è‡ªå‹•çš„ã«ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ã¾ã™
# LangChainæœªè¨­å®šæ™‚ã¯å®šå‹æ–‡ã‚’ç”Ÿæˆï¼ˆè­¦å‘Šä»˜ãï¼‰
```

---

**å•é¡Œ4: `Timeout error: Request timed out`**

**åŸå› **: ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šãŒé…ã„ã€ã¾ãŸã¯OpenAI APIã‚µãƒ¼ãƒãƒ¼ã®å¿œç­”ãŒé…å»¶

**è§£æ±ºç­–**:
```python
# ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ™‚é–“ã‚’å»¶é•·
llm = ChatOpenAI(
    model_name="gpt-3.5-turbo",
    temperature=0.7,
    openai_api_key=api_key,
    request_timeout=60  # 30ç§’ â†’ 60ç§’ã«å»¶é•·
)
```

---

**å•é¡Œ5: `ModuleNotFoundError: No module named 'langchain'`**

**åŸå› **: LangChainãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ãªã„ã€ã¾ãŸã¯ä»®æƒ³ç’°å¢ƒãŒç•°ãªã‚‹

**è§£æ±ºç­–**:
```bash
# ç¾åœ¨ã®Pythonç’°å¢ƒã‚’ç¢ºèª
which python3
python3 --version

# LangChainã‚’å†ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install --upgrade langchain==0.1.0 openai==1.12.0

# ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ç¢ºèª
python3 -c "import langchain; print(langchain.__version__)"
```

---

**å•é¡Œ6: `JSONDecodeError: Expecting value`**

**åŸå› **: OpenAI APIã‹ã‚‰ã®å¿œç­”ãŒæœŸå¾…ã•ã‚Œã‚‹JSONå½¢å¼ã§ã¯ãªã„ï¼ˆç¨€ãªã‚±ãƒ¼ã‚¹ï¼‰

**è§£æ±ºç­–**:
```python
# ãƒªãƒˆãƒ©ã‚¤ãƒ­ã‚¸ãƒƒã‚¯ã§å¯¾å¿œï¼ˆapp_improved.pyã«å®Ÿè£…æ¸ˆã¿ï¼‰
# æœ€å¤§3å›ãƒªãƒˆãƒ©ã‚¤ â†’ å¤±æ•—æ™‚ã¯ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
```

#### E.3 ã‚³ã‚¹ãƒˆç®¡ç†ã‚¬ã‚¤ãƒ‰

**GPT-3.5-turbo æ–™é‡‘ï¼ˆ2025å¹´1æœˆæ™‚ç‚¹ï¼‰**:
- Input: $0.0005 / 1K tokens (ç´„Â¥0.075 / 1K tokens, 1ãƒ‰ãƒ«=150å††æ›ç®—)
- Output: $0.0015 / 1K tokens (ç´„Â¥0.225 / 1K tokens)

**Q-Storm Platform ä½¿ç”¨ãƒ‘ã‚¿ãƒ¼ãƒ³ã”ã¨ã®æ¨å®šã‚³ã‚¹ãƒˆ**:

| ã‚·ãƒŠãƒªã‚ª | Input Tokens | Output Tokens | API Callå˜ä¾¡ | æœˆé–“100å›å®Ÿè¡Œæ™‚ã‚³ã‚¹ãƒˆ |
|----------|--------------|---------------|-------------|---------------------|
| **EDAæ´å¯Ÿç”Ÿæˆ** (æ¨™æº–ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ) | ~2,500 | ~800 | $0.0025 | $0.25 (ç´„Â¥38) |
| **ãƒ‘ãƒ¬ãƒ¼ãƒˆè§£èª¬ç”Ÿæˆ** | ~1,500 | ~600 | $0.0018 | $0.18 (ç´„Â¥27) |
| **å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ EDA** | ~4,000 | ~1,200 | $0.0038 | $0.38 (ç´„Â¥57) |

**æ¨å®šæœˆé–“ã‚³ã‚¹ãƒˆ**:
- **è»½é‡ä½¿ç”¨** (é€±10å›EDAå®Ÿè¡Œ): ç´„Â¥152 (~$1.00)
- **æ¨™æº–ä½¿ç”¨** (æ—¥1å›EDAå®Ÿè¡Œ): ç´„Â¥1,140 (~$7.60)
- **ãƒ˜ãƒ“ãƒ¼ä½¿ç”¨** (æ—¥5å›EDAå®Ÿè¡Œ): ç´„Â¥5,700 (~$38.00)

**ã‚³ã‚¹ãƒˆå‰Šæ¸›æˆ¦ç•¥**:

1. **LRUã‚­ãƒ£ãƒƒã‚·ãƒ¥æ´»ç”¨** (app_improved.pyã«å®Ÿè£…æ¸ˆã¿):
   - åŒä¸€ãƒ‡ãƒ¼ã‚¿ãƒ»åŒä¸€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰è¿”å´
   - TTL: 1æ™‚é–“ã€æœ€å¤§100ã‚¨ãƒ³ãƒˆãƒª
   - **æœŸå¾…å‰Šæ¸›åŠ¹æœ**: 30-50%

2. **ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯**:
   - é–‹ç™ºç’°å¢ƒã‚„ãƒ‡ãƒ¢ç’°å¢ƒã§ã¯`OPENAI_API_KEY`ã‚’è¨­å®šã›ãšãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®ã¿ä½¿ç”¨
   - **å‰Šæ¸›åŠ¹æœ**: 100% (APIå‘¼ã³å‡ºã—ã‚¼ãƒ­)

3. **ä½¿ç”¨åˆ¶é™è¨­å®š**:
   ```bash
   # OpenAI Platform > Billing > Limits ã§æœˆé–“ä¸Šé™ã‚’è¨­å®š
   # ä¾‹: æœˆ$10ä¸Šé™ â†’ ç´„400å›ã®EDAå®Ÿè¡ŒãŒå¯èƒ½
   ```

4. **ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæœ€é©åŒ–**:
   - ä¸è¦ãªå¤‰æ•°ã‚’å‰Šé™¤ã—ã¦Input Tokenæ•°ã‚’å‰Šæ¸›
   - å‡ºåŠ›å½¢å¼ã‚’ç°¡æ½”ã«ï¼ˆ`max_tokens`ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§åˆ¶å¾¡ï¼‰

#### E.4 æœ¬ç•ªç’°å¢ƒãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆ

**Step 1: APIã‚­ãƒ¼ã®å®‰å…¨ãªç®¡ç†**

```bash
# âŒ æ‚ªã„ä¾‹: APIã‚­ãƒ¼ã‚’ã‚³ãƒ¼ãƒ‰ã«åŸ‹ã‚è¾¼ã¿
api_key = "sk-proj-xxxxxxxxxxxxxxxxxxxx"  # NEVER DO THIS

# âœ… è‰¯ã„ä¾‹: ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—
api_key = os.environ.get('OPENAI_API_KEY', '')
```

**Step 2: .gitignoreè¨­å®š**

```.gitignore
# API Keys and secrets
.env
.env.local
.env.production

# Credentials
credentials.json
openai_key.txt
```

**Step 3: CI/CDãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³è¨­å®š**

```yaml
# .github/workflows/deploy.yml (GitHub Actions example)

name: Deploy to Production

on:
  push:
    branches: [main]

jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2

      - name: Set up Python
        uses: actions/setup-python@v2
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install -r requirements.txt

      - name: Verify LangChain setup
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        run: |
          python3 test_langchain_setup.py

      - name: Deploy to server
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        run: |
          # ãƒ‡ãƒ—ãƒ­ã‚¤ã‚¹ã‚¯ãƒªãƒ—ãƒˆå®Ÿè¡Œ
          ./deploy.sh
```

**Step 4: ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°**

```python
# app_improved.py ã«LangChainä½¿ç”¨çµ±è¨ˆãƒ­ã‚®ãƒ³ã‚°ã‚’è¿½åŠ 

import logging

langchain_usage_logger = logging.getLogger('langchain_usage')
langchain_usage_logger.setLevel(logging.INFO)

handler = logging.FileHandler('logs/langchain_usage.log')
handler.setFormatter(logging.Formatter('%(asctime)s - %(message)s'))
langchain_usage_logger.addHandler(handler)

# LangChain APIå‘¼ã³å‡ºã—æ™‚ã«ãƒ­ã‚°è¨˜éŒ²
def log_langchain_usage(prompt_tokens, completion_tokens, cache_hit=False):
    langchain_usage_logger.info(
        f"API_Call | Prompt: {prompt_tokens} tokens | "
        f"Completion: {completion_tokens} tokens | "
        f"Cache: {'HIT' if cache_hit else 'MISS'} | "
        f"Estimated_Cost: ${(prompt_tokens * 0.0005 + completion_tokens * 0.0015) / 1000:.4f}"
    )
```

**å®šæœŸãƒ¬ãƒ“ãƒ¥ãƒ¼**:
```bash
# æœˆé–“ä½¿ç”¨çµ±è¨ˆã®ç¢ºèª
cat logs/langchain_usage.log | grep "API_Call" | wc -l  # APIå‘¼ã³å‡ºã—å›æ•°
grep "Estimated_Cost" logs/langchain_usage.log | awk '{sum+=$NF} END {print "Total: $"sum}'  # ç·ã‚³ã‚¹ãƒˆ
```

#### E.5 ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

**é–‹ç™ºç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—**:
- [ ] OpenAI APIã‚­ãƒ¼å–å¾—å®Œäº†
- [ ] ç’°å¢ƒå¤‰æ•° `OPENAI_API_KEY` è¨­å®šå®Œäº†
- [ ] `pip install langchain openai` å®Œäº†
- [ ] `test_langchain_setup.py` å®Ÿè¡ŒæˆåŠŸ
- [ ] app_improved.py èµ·å‹•æ™‚ã«LangChainåˆæœŸåŒ–ãƒ­ã‚°ç¢ºèª

**æœ¬ç•ªç’°å¢ƒãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆå‰**:
- [ ] APIã‚­ãƒ¼ãŒ `.gitignore` ã«è¿½åŠ ã•ã‚Œã¦ã„ã‚‹
- [ ] ç’°å¢ƒå¤‰æ•°ãŒCI/CD Secretsã«è¨­å®šã•ã‚Œã¦ã„ã‚‹
- [ ] ä½¿ç”¨åˆ¶é™ï¼ˆæœˆé–“ä¸Šé™ï¼‰ãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹
- [ ] ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ãƒ»ãƒ­ã‚®ãƒ³ã‚°ãŒæœ‰åŠ¹åŒ–ã•ã‚Œã¦ã„ã‚‹
- [ ] ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ©Ÿæ§‹ã®å‹•ä½œç¢ºèªå®Œäº†

**ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°å‚ç…§æ¸ˆã¿**:
- [ ] Appendix E.2 ã®ã‚ˆãã‚ã‚‹å•é¡Œã‚’ç¢ºèªæ¸ˆã¿
- [ ] ã‚³ã‚¹ãƒˆç®¡ç†ã‚¬ã‚¤ãƒ‰ï¼ˆE.3ï¼‰ã‚’ç†è§£æ¸ˆã¿
- [ ] æœ¬ç•ªç’°å¢ƒãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆæ‰‹é †ï¼ˆE.4ï¼‰ã‚’ç¢ºèªæ¸ˆã¿

### Appendix F: Renderãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆã‚¬ã‚¤ãƒ‰

ã“ã®Appendixã¯ã€Q-Storm Platformã‚’Render (https://render.com) ã«ãƒ‡ãƒ—ãƒ­ã‚¤ã™ã‚‹ãŸã‚ã®å®Œå…¨ã‚¬ã‚¤ãƒ‰ã§ã™ã€‚Renderã¯ã€Herokuã®ä»£æ›¿ã¨ã—ã¦äººæ°—ã®PaaSãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã§ã€ç„¡æ–™ãƒ—ãƒ©ãƒ³ã‹ã‚‰ã‚¹ã‚¿ãƒ¼ãƒˆã§ãã€è‡ªå‹•ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã€PostgreSQLçµ±åˆã€GitHubé€£æºãŒæ¨™æº–è£…å‚™ã•ã‚Œã¦ã„ã¾ã™ã€‚

#### F.1 Renderè¨­å®š

**ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹æˆè¦ä»¶**:
```
Q-Storm-Project3/
â”œâ”€â”€ app_improved.py          # ãƒ¡ã‚¤ãƒ³ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
â”œâ”€â”€ requirements.txt         # Pythonä¾å­˜é–¢ä¿‚
â”œâ”€â”€ render.yaml              # Renderè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆInfrastructure as Codeï¼‰
â”œâ”€â”€ .gitignore               # Gité™¤å¤–è¨­å®š
â”œâ”€â”€ README.md
â”œâ”€â”€ uploads/                 # ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ï¼ˆ.gitignoreè¿½åŠ ï¼‰
â””â”€â”€ outputs/                 # ç”Ÿæˆãƒ¬ãƒãƒ¼ãƒˆï¼ˆ.gitignoreè¿½åŠ ï¼‰
```

**render.yaml ã‚µãƒ³ãƒ—ãƒ«è¨­å®š**:

```yaml
# render.yaml - Render Infrastructure as Code
# ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒªãƒã‚¸ãƒˆãƒªãƒ«ãƒ¼ãƒˆã«é…ç½®

services:
  # Web Service - Flask Application
  - type: web
    name: qstorm-platform
    env: python
    region: singapore  # ã¾ãŸã¯ oregon (åœ°ç†çš„ã«è¿‘ã„ãƒªãƒ¼ã‚¸ãƒ§ãƒ³ã‚’é¸æŠ)
    plan: starter      # ç„¡æ–™: free, æœ‰æ–™: starter ($7/month), standard ($25/month)
    branch: main       # ãƒ‡ãƒ—ãƒ­ã‚¤å¯¾è±¡ãƒ–ãƒ©ãƒ³ãƒ
    buildCommand: pip install -r requirements.txt
    startCommand: python3 app_improved.py

    # ç’°å¢ƒå¤‰æ•°ï¼ˆRender Dashboard ã§å®Ÿéš›ã®å€¤ã‚’è¨­å®šï¼‰
    envVars:
      - key: OPENAI_API_KEY
        sync: false  # DashboardçµŒç”±ã§æ‰‹å‹•è¨­å®šï¼ˆSecretã¨ã—ã¦æ‰±ã†ï¼‰

      - key: PORT
        value: 5003

      - key: FLASK_ENV
        value: production

      - key: DATABASE_URL
        fromDatabase:
          name: qstorm-db
          property: connectionString

      - key: MAX_FILE_SIZE_MB
        value: 200

      - key: SESSION_SECRET_KEY
        generateValue: true  # RenderãŒè‡ªå‹•ç”Ÿæˆ

      - key: PYTHONUNBUFFERED
        value: 1  # ãƒ­ã‚°å³æ™‚å‡ºåŠ›

    # ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
    healthCheckPath: /api/v2/health

    # è‡ªå‹•ãƒ‡ãƒ—ãƒ­ã‚¤
    autoDeploy: true

    # ãƒ‡ã‚£ã‚¹ã‚¯ï¼ˆæ°¸ç¶šåŒ–ãŒå¿…è¦ãªå ´åˆï¼‰
    # Note: ç„¡æ–™ãƒ—ãƒ©ãƒ³ã§ã¯ãƒ‡ã‚£ã‚¹ã‚¯æ°¸ç¶šåŒ–ä¸å¯
    # disk:
    #   name: qstorm-data
    #   mountPath: /opt/render/project/src/uploads
    #   sizeGB: 1

  # Background Worker (ã‚ªãƒ—ã‚·ãƒ§ãƒ³ - Celeryã‚¿ã‚¹ã‚¯ç”¨)
  # - type: worker
  #   name: qstorm-worker
  #   env: python
  #   branch: main
  #   buildCommand: pip install -r requirements.txt
  #   startCommand: celery -A app_improved.celery worker --loglevel=info

# Database - PostgreSQL
databases:
  - name: qstorm-db
    databaseName: qstorm_production
    user: qstorm_user
    plan: starter  # ç„¡æ–™: free (90æ—¥å¾Œå‰Šé™¤), æœ‰æ–™: starter ($7/month)
    region: singapore
    ipAllowList: []  # ç©º = ã™ã¹ã¦ã®IPã‹ã‚‰æ¥ç¶šå¯èƒ½ï¼ˆWeb Serviceã®ã¿ã‚¢ã‚¯ã‚»ã‚¹ï¼‰

# Redis (ã‚ªãƒ—ã‚·ãƒ§ãƒ³ - ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°ç”¨)
# - type: redis
#   name: qstorm-redis
#   plan: starter
#   region: singapore
#   maxmemoryPolicy: allkeys-lru
#   ipAllowList: []
```

**requirements.txt æ›´æ–°**:

```txt
# requirements.txt - Renderç”¨ã«æœ€é©åŒ–

# Core Framework
Flask==3.0.0
flask-cors==4.0.0
Werkzeug==3.0.1

# Data Processing
pandas==2.1.4
numpy==1.26.2
openpyxl==3.1.2

# Analysis & Visualization
scipy==1.11.4
matplotlib==3.8.2
sweetviz==2.3.1

# NLP/LLM
langchain==0.1.0
openai==1.12.0

# System Monitoring
psutil==5.9.6

# Production Server
gunicorn==21.2.0      # æœ¬ç•ªç”¨WSGIã‚µãƒ¼ãƒãƒ¼ï¼ˆRenderã§æ¨å¥¨ï¼‰

# Database (Phase 2ä»¥é™ã§æœ‰åŠ¹åŒ–)
# psycopg2-binary==2.9.9  # PostgreSQL adapter
# SQLAlchemy==2.0.23      # ORM

# Caching (Phase 2ä»¥é™ã§æœ‰åŠ¹åŒ–)
# redis==5.0.1
# celery==5.3.4

# Environment Management
python-dotenv==1.0.0
```

**ãƒ“ãƒ«ãƒ‰ã‚³ãƒãƒ³ãƒ‰è©³ç´°**:

```bash
# RenderãŒå®Ÿè¡Œã™ã‚‹ãƒ“ãƒ«ãƒ‰ãƒ—ãƒ­ã‚»ã‚¹

# Step 1: Pythonç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ï¼ˆè‡ªå‹•ï¼‰
# Python 3.11ãŒè‡ªå‹•ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã‚‹

# Step 2: ä¾å­˜é–¢ä¿‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install -r requirements.txt

# Step 3: é™çš„ãƒ•ã‚¡ã‚¤ãƒ«åé›†ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
# mkdir -p outputs uploads

# Step 4: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆPhase 2ä»¥é™ï¼‰
# python3 manage.py db upgrade
```

**ã‚¹ã‚¿ãƒ¼ãƒˆã‚³ãƒãƒ³ãƒ‰è©³ç´°**:

```bash
# Option 1: é–‹ç™ºã‚µãƒ¼ãƒãƒ¼ï¼ˆå°è¦æ¨¡ãƒ»ãƒ†ã‚¹ãƒˆç”¨ï¼‰
python3 app_improved.py

# Option 2: Gunicornï¼ˆæœ¬ç•ªæ¨å¥¨ - 4ãƒ¯ãƒ¼ã‚«ãƒ¼ï¼‰
gunicorn --bind 0.0.0.0:$PORT --workers 4 --timeout 120 app_improved:app

# Option 3: Gunicorn with éåŒæœŸãƒ¯ãƒ¼ã‚«ãƒ¼ï¼ˆé«˜è² è·å¯¾å¿œï¼‰
gunicorn --bind 0.0.0.0:$PORT --workers 4 --worker-class gevent --timeout 120 app_improved:app
```

**æ¨å¥¨ã‚¹ã‚¿ãƒ¼ãƒˆã‚³ãƒãƒ³ãƒ‰** (render.yamlã«è¨˜è¼‰):
```yaml
startCommand: gunicorn --bind 0.0.0.0:$PORT --workers 2 --timeout 120 app_improved:app
```

**ç’°å¢ƒå¤‰æ•°ãƒªã‚¹ãƒˆï¼ˆå®Œå…¨ç‰ˆï¼‰**:

| ç’°å¢ƒå¤‰æ•°å | å¿…é ˆ | ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ | èª¬æ˜ | Renderè¨­å®šæ–¹æ³• |
|-----------|------|-------------|------|---------------|
| `OPENAI_API_KEY` | âŒ (Phase 1ã§æ¨å¥¨) | ãªã— | OpenAI APIèªè¨¼ã‚­ãƒ¼ | Dashboard > Environment > Add Secret |
| `PORT` | âœ… | 5003 | ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒãƒ¼ãƒˆ | è‡ªå‹•è¨­å®šï¼ˆRenderãŒæ³¨å…¥ï¼‰ |
| `FLASK_ENV` | âœ… | production | Flaskç’°å¢ƒãƒ¢ãƒ¼ãƒ‰ | `production` å›ºå®š |
| `DATABASE_URL` | âŒ (Phase 2ã§å¿…é ˆ) | ãªã— | PostgreSQLæ¥ç¶šURL | è‡ªå‹•è¨­å®šï¼ˆDBé€£æºæ™‚ï¼‰ |
| `SESSION_SECRET_KEY` | âœ… | ãƒ©ãƒ³ãƒ€ãƒ ç”Ÿæˆ | Flask sessionæš—å·åŒ–ã‚­ãƒ¼ | Generate Valueæœ‰åŠ¹åŒ– |
| `MAX_FILE_SIZE_MB` | âŒ | 200 | æœ€å¤§ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º | 200-500 |
| `PYTHONUNBUFFERED` | âœ… | 1 | ãƒ­ã‚°ãƒãƒƒãƒ•ã‚¡ãƒªãƒ³ã‚°ç„¡åŠ¹åŒ– | `1` å›ºå®š |
| `REDIS_URL` | âŒ (Phase 2) | ãªã— | Redisã‚­ãƒ£ãƒƒã‚·ãƒ¥æ¥ç¶šURL | è‡ªå‹•è¨­å®šï¼ˆRedisé€£æºæ™‚ï¼‰ |

#### F.2 GitHubé€£æº

**ãƒªãƒã‚¸ãƒˆãƒªæ§‹æˆè¦ä»¶**:

```
GitHub Repository: Q-Storm-Platform
â”œâ”€â”€ main branch          # æœ¬ç•ªç’°å¢ƒï¼ˆRenderè‡ªå‹•ãƒ‡ãƒ—ãƒ­ã‚¤ï¼‰
â”œâ”€â”€ develop branch       # é–‹ç™ºç’°å¢ƒï¼ˆã‚¹ãƒ†ãƒ¼ã‚¸ãƒ³ã‚°ï¼‰
â””â”€â”€ feature/* branches   # æ©Ÿèƒ½é–‹ç™ºãƒ–ãƒ©ãƒ³ãƒ
```

**æ¨å¥¨ãƒ–ãƒ©ãƒ³ãƒæˆ¦ç•¥**:

```
main (protected)
  â†‘ Pull Request + Review Required
develop
  â†‘ Pull Request
feature/eda-langchain-integration
feature/frontend-dashboard
hotfix/critical-bug-fix
```

**GitHubé€£æºæ‰‹é †**:

**Step 1: Renderã‚¢ã‚«ã‚¦ãƒ³ãƒˆä½œæˆã¨GitHubæ¥ç¶š**

```bash
# 1. Render (https://render.com) ã§ã‚¢ã‚«ã‚¦ãƒ³ãƒˆä½œæˆ
# 2. "Connect GitHub" ã‚’ã‚¯ãƒªãƒƒã‚¯
# 3. ãƒªãƒã‚¸ãƒˆãƒªã‚¢ã‚¯ã‚»ã‚¹æ¨©é™ã‚’æ‰¿èª
# 4. "Q-Storm-Platform" ãƒªãƒã‚¸ãƒˆãƒªã‚’é¸æŠ
```

**Step 2: è‡ªå‹•ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆè¨­å®š**

Render Dashboard:
```
1. New > Web Service
2. Connect Repository: Q-Storm-Platform
3. Branch: main (æœ¬ç•ª) ã¾ãŸã¯ develop (ã‚¹ãƒ†ãƒ¼ã‚¸ãƒ³ã‚°)
4. Build Command: pip install -r requirements.txt
5. Start Command: gunicorn --bind 0.0.0.0:$PORT --workers 2 --timeout 120 app_improved:app
6. Auto-Deploy: Yes (mainãƒ–ãƒ©ãƒ³ãƒã¸ã®push/mergeã§è‡ªå‹•ãƒ‡ãƒ—ãƒ­ã‚¤)
```

**Step 3: ãƒ—ãƒ«ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ - æœ‰æ–™ãƒ—ãƒ©ãƒ³ï¼‰**

```yaml
# render.yaml ã«è¿½åŠ 

services:
  - type: web
    name: qstorm-platform
    # ... (æ—¢å­˜è¨­å®š)

    # Pull Request Previews (Standardãƒ—ãƒ©ãƒ³ä»¥ä¸Š)
    previewsEnabled: true
    previewsExpireAfterDays: 7
```

ã“ã‚Œã«ã‚ˆã‚Šã€PRã”ã¨ã«ä¸€æ™‚çš„ãªãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ç’°å¢ƒãŒè‡ªå‹•ç”Ÿæˆã•ã‚Œã¾ã™ã€‚

**Step 4: .gitignore è¨­å®š**

```gitignore
# .gitignore - Renderãƒ‡ãƒ—ãƒ­ã‚¤ç”¨

# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
venv/
env/
ENV/

# Flask
instance/
.webassets-cache

# Data & Uploads (Renderã§ã¯æ°¸ç¶šåŒ–ã•ã‚Œãªã„)
uploads/*
!uploads/.gitkeep
outputs/*
!outputs/.gitkeep

# Logs
*.log
logs/

# Environment Variables
.env
.env.local
.env.production

# API Keys & Secrets
credentials.json
openai_key.txt

# OS Files
.DS_Store
Thumbs.db

# IDE
.vscode/
.idea/
*.swp

# Test Coverage
htmlcov/
.coverage
.pytest_cache/

# Database
*.db
*.sqlite3

# Backups
*.backup
IMPLEMENTATION_WORKFLOW.md.backup
```

**Step 5: GitHub Actions CI/CDï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰**

```yaml
# .github/workflows/render-deploy.yml

name: Deploy to Render

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install -r requirements.txt

      - name: Run tests
        run: |
          python3 -m pytest test_pareto_system.py -v

      - name: Verify LangChain setup (mock)
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        run: |
          python3 test_langchain_setup.py || echo "LangChain test skipped (no API key)"

  deploy:
    needs: test
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    steps:
      - name: Deploy to Render
        run: |
          echo "Render auto-deploys on git push to main"
          echo "No manual deployment needed"
```

#### F.3 ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨­å®š

**PostgreSQLæ¥ç¶šè¨­å®šï¼ˆPhase 2ä»¥é™ï¼‰**:

**Step 1: Render PostgreSQLä½œæˆ**

```bash
# Render Dashboard
1. New > PostgreSQL
2. Name: qstorm-db
3. Database: qstorm_production
4. User: qstorm_user (è‡ªå‹•ç”Ÿæˆ)
5. Plan: Starter ($7/month) ã¾ãŸã¯ Free (90æ—¥åˆ¶é™)
6. Region: Singapore (Web Serviceã¨åŒã˜ãƒªãƒ¼ã‚¸ãƒ§ãƒ³æ¨å¥¨)
```

**Step 2: æ¥ç¶šURLå–å¾—**

RenderãŒè‡ªå‹•ç”Ÿæˆã™ã‚‹ç’°å¢ƒå¤‰æ•°:
```bash
DATABASE_URL=postgresql://qstorm_user:password@dpg-xxxxx.singapore-postgres.render.com/qstorm_production

# å†…éƒ¨æ¥ç¶šURLï¼ˆã‚ˆã‚Šé«˜é€Ÿ - åŒãƒªãƒ¼ã‚¸ãƒ§ãƒ³å†…é€šä¿¡ï¼‰
DATABASE_URL_INTERNAL=postgresql://qstorm_user:password@dpg-xxxxx/qstorm_production
```

**Step 3: app_improved.py ã§ã®DBæ¥ç¶š**

```python
# app_improved.py - PostgreSQLçµ±åˆï¼ˆPhase 2ï¼‰

import os
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker

# ç’°å¢ƒå¤‰æ•°ã‹ã‚‰DB URLå–å¾—ï¼ˆRenderãŒè‡ªå‹•æ³¨å…¥ï¼‰
DATABASE_URL = os.environ.get('DATABASE_URL')

if DATABASE_URL:
    # Renderã®å†…éƒ¨URLå„ªå…ˆï¼ˆé«˜é€ŸåŒ–ï¼‰
    DATABASE_URL_INTERNAL = os.environ.get('DATABASE_URL_INTERNAL', DATABASE_URL)

    # SQLAlchemy engineä½œæˆ
    engine = create_engine(
        DATABASE_URL_INTERNAL,
        pool_size=5,              # æ¥ç¶šãƒ—ãƒ¼ãƒ«ã‚µã‚¤ã‚º
        max_overflow=10,          # æœ€å¤§è¿½åŠ æ¥ç¶šæ•°
        pool_pre_ping=True,       # æ¥ç¶šæœ‰åŠ¹æ€§ãƒã‚§ãƒƒã‚¯
        pool_recycle=3600,        # 1æ™‚é–“ã§æ¥ç¶šãƒªã‚µã‚¤ã‚¯ãƒ«
        echo=False                # SQLãƒ­ã‚°ç„¡åŠ¹åŒ–ï¼ˆæœ¬ç•ªç’°å¢ƒï¼‰
    )

    Session = sessionmaker(bind=engine)
    logger.info(f'PostgreSQLæ¥ç¶šæˆåŠŸ: {DATABASE_URL_INTERNAL.split("@")[1]}')
else:
    logger.warning('DATABASE_URLæœªè¨­å®š - ãƒ•ã‚¡ã‚¤ãƒ«ãƒ™ãƒ¼ã‚¹ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‚’ä½¿ç”¨')
    engine = None
```

**ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æˆ¦ç•¥**:

**Option 1: Alembicï¼ˆæ¨å¥¨ - Phase 2ï¼‰**

```bash
# Alembicã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
pip install alembic

# åˆæœŸåŒ–
alembic init migrations

# ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
alembic revision --autogenerate -m "Initial schema"

# æœ¬ç•ªé©ç”¨ï¼ˆRenderãƒ“ãƒ«ãƒ‰ã‚³ãƒãƒ³ãƒ‰ã«è¿½åŠ ï¼‰
alembic upgrade head
```

**render.yaml ã«ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³è¿½åŠ **:
```yaml
services:
  - type: web
    name: qstorm-platform
    buildCommand: |
      pip install -r requirements.txt
      alembic upgrade head  # ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³è‡ªå‹•å®Ÿè¡Œ
    startCommand: gunicorn --bind 0.0.0.0:$PORT --workers 2 app_improved:app
```

**Option 2: æ‰‹å‹•SQLå®Ÿè¡Œï¼ˆPhase 1 - ã‚·ãƒ³ãƒ—ãƒ«ï¼‰**

```sql
-- Render Dashboard > Database > Query ã§å®Ÿè¡Œ

CREATE TABLE IF NOT EXISTS analysis_sessions (
    session_id VARCHAR(50) PRIMARY KEY,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    store VARCHAR(20),
    file_name VARCHAR(255),
    row_count INTEGER,
    column_count INTEGER
);

CREATE INDEX idx_sessions_created ON analysis_sessions(created_at);
```

**ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—**:

```bash
# Render Dashboard > Database > Backups
# è‡ªå‹•ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—: æ¯æ—¥ï¼ˆStarterãƒ—ãƒ©ãƒ³ä»¥ä¸Šï¼‰
# æ‰‹å‹•ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—: "Create Backup" ãƒœã‚¿ãƒ³

# ãƒ­ãƒ¼ã‚«ãƒ«ã¸ã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
pg_dump $DATABASE_URL > qstorm_backup_$(date +%Y%m%d).sql

# ãƒªã‚¹ãƒˆã‚¢
psql $DATABASE_URL < qstorm_backup_20250120.sql
```

#### F.4 ç’°å¢ƒå¤‰æ•°ç®¡ç†

**Render Dashboardã§ã®ç’°å¢ƒå¤‰æ•°è¨­å®šæ‰‹é †**:

**Step 1: Web Service > Environment ã‚¿ãƒ–**

```
1. Environment Variables ã‚»ã‚¯ã‚·ãƒ§ãƒ³
2. "Add Environment Variable" ã‚¯ãƒªãƒƒã‚¯
3. Key/Valueå…¥åŠ›
4. Secret (æ©Ÿå¯†æƒ…å ±) ã®å ´åˆã¯ "Secret" ã«ãƒã‚§ãƒƒã‚¯
```

**Step 2: å¿…é ˆç’°å¢ƒå¤‰æ•°ã®è¨­å®š**

**OpenAI APIã‚­ãƒ¼ï¼ˆLangChainçµ±åˆç”¨ï¼‰**:
```
Key: OPENAI_API_KEY
Value: sk-proj-xxxxxxxxxxxxxxxxxxxxxxxxxxxx
Type: Secret âœ…
```

**ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹URLï¼ˆPhase 2 - è‡ªå‹•è¨­å®šï¼‰**:
```
Key: DATABASE_URL
Value: (Render PostgreSQLé€£æºæ™‚ã«è‡ªå‹•è¨­å®š)
Type: Secret âœ…
```

**Flaskã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚·ãƒ¼ã‚¯ãƒ¬ãƒƒãƒˆ**:
```
Key: SESSION_SECRET_KEY
Value: (Generate Valueã§è‡ªå‹•ç”Ÿæˆ)
Type: Secret âœ…
```

**ãã®ä»–ã®æ¨å¥¨è¨­å®š**:
```
Key: MAX_FILE_SIZE_MB
Value: 200
Type: Plain

Key: FLASK_ENV
Value: production
Type: Plain

Key: PYTHONUNBUFFERED
Value: 1
Type: Plain

Key: LOG_LEVEL
Value: INFO
Type: Plain
```

**ç’°å¢ƒå¤‰æ•°ã®æ¤œè¨¼ã‚¹ã‚¯ãƒªãƒ—ãƒˆ**:

```python
# verify_env.py - ãƒ‡ãƒ—ãƒ­ã‚¤å‰ã®ç’°å¢ƒå¤‰æ•°ãƒã‚§ãƒƒã‚¯

import os
import sys

def verify_environment():
    """å¿…é ˆç’°å¢ƒå¤‰æ•°ã®å­˜åœ¨ç¢ºèª"""

    required_vars = {
        'PORT': 'ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒãƒ¼ãƒˆ',
        'FLASK_ENV': 'Flaskç’°å¢ƒãƒ¢ãƒ¼ãƒ‰',
        'SESSION_SECRET_KEY': 'ã‚»ãƒƒã‚·ãƒ§ãƒ³æš—å·åŒ–ã‚­ãƒ¼',
    }

    recommended_vars = {
        'OPENAI_API_KEY': 'LangChainçµ±åˆï¼ˆPhase 1æ¨å¥¨ï¼‰',
        'DATABASE_URL': 'PostgreSQLæ¥ç¶šï¼ˆPhase 2å¿…é ˆï¼‰',
    }

    missing_required = []
    missing_recommended = []

    print("=" * 60)
    print("ç’°å¢ƒå¤‰æ•°æ¤œè¨¼")
    print("=" * 60)

    # å¿…é ˆå¤‰æ•°ãƒã‚§ãƒƒã‚¯
    for var, desc in required_vars.items():
        if os.environ.get(var):
            print(f"âœ… {var}: {desc}")
        else:
            print(f"âŒ {var}: {desc} - æœªè¨­å®š")
            missing_required.append(var)

    # æ¨å¥¨å¤‰æ•°ãƒã‚§ãƒƒã‚¯
    for var, desc in recommended_vars.items():
        if os.environ.get(var):
            print(f"âœ… {var}: {desc}")
        else:
            print(f"âš ï¸  {var}: {desc} - æœªè¨­å®šï¼ˆæ¨å¥¨ï¼‰")
            missing_recommended.append(var)

    print("=" * 60)

    if missing_required:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: å¿…é ˆç’°å¢ƒå¤‰æ•°ãŒæœªè¨­å®šã§ã™: {', '.join(missing_required)}")
        sys.exit(1)

    if missing_recommended:
        print(f"âš ï¸  è­¦å‘Š: æ¨å¥¨ç’°å¢ƒå¤‰æ•°ãŒæœªè¨­å®šã§ã™: {', '.join(missing_recommended)}")
        print("   ä¸€éƒ¨æ©Ÿèƒ½ãŒåˆ¶é™ã•ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™")

    print("âœ… ç’°å¢ƒå¤‰æ•°æ¤œè¨¼å®Œäº†")
    return True

if __name__ == '__main__':
    verify_environment()
```

**ãƒ“ãƒ«ãƒ‰ãƒ—ãƒ­ã‚»ã‚¹ã«çµ±åˆ**:
```yaml
# render.yaml
services:
  - type: web
    buildCommand: |
      pip install -r requirements.txt
      python3 verify_env.py
```

#### F.5 ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã¨ãƒ­ã‚°

**Renderãƒ­ã‚°ç¢ºèªæ–¹æ³•**:

**ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ­ã‚°è¡¨ç¤º**:
```bash
# Render Dashboard > Logs ã‚¿ãƒ–
# ã¾ãŸã¯ CLIçµŒç”±ï¼ˆrender-cliã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å¾Œï¼‰

render logs -s qstorm-platform --tail 100
```

**ãƒ­ã‚°ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°**:
```
# Dashboard > Logs > Search
# æ¤œç´¢ä¾‹:
ERROR                    # ã‚¨ãƒ©ãƒ¼ã®ã¿è¡¨ç¤º
LangChain               # LangChainé–¢é€£ãƒ­ã‚°
APIå‘¼ã³å‡ºã—             # æ—¥æœ¬èªãƒ­ã‚°æ¤œç´¢
status=500              # HTTPã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰
```

**ãƒ­ã‚°ã®æ°¸ç¶šåŒ–ï¼ˆæ¨å¥¨ï¼‰**:

```python
# app_improved.py - ãƒ­ã‚°è¨­å®šå¼·åŒ–

import logging
from logging.handlers import RotatingFileHandler
import os

# ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
os.makedirs('logs', exist_ok=True)

# ãƒ­ã‚¬ãƒ¼è¨­å®š
logger = logging.getLogger('qstorm')
logger.setLevel(logging.INFO)

# ã‚³ãƒ³ã‚½ãƒ¼ãƒ«å‡ºåŠ›ï¼ˆRenderãƒ­ã‚°ï¼‰
console_handler = logging.StreamHandler()
console_handler.setLevel(logging.INFO)
console_formatter = logging.Formatter(
    '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
console_handler.setFormatter(console_formatter)

# ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›ï¼ˆãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³: 10MB x 5ãƒ•ã‚¡ã‚¤ãƒ«ï¼‰
file_handler = RotatingFileHandler(
    'logs/qstorm.log',
    maxBytes=10*1024*1024,  # 10MB
    backupCount=5
)
file_handler.setLevel(logging.INFO)
file_handler.setFormatter(console_formatter)

logger.addHandler(console_handler)
logger.addHandler(file_handler)

# ä½¿ç”¨ä¾‹
logger.info('Application started')
logger.error('Database connection failed', exc_info=True)
```

**æ§‹é€ åŒ–ãƒ­ã‚°ï¼ˆJSONå½¢å¼ï¼‰**:

```python
import json
import logging

class JSONFormatter(logging.Formatter):
    """JSONå½¢å¼ã®ãƒ­ã‚°ãƒ•ã‚©ãƒ¼ãƒãƒƒã‚¿ãƒ¼"""

    def format(self, record):
        log_data = {
            'timestamp': self.formatTime(record),
            'level': record.levelname,
            'logger': record.name,
            'message': record.getMessage(),
            'module': record.module,
            'function': record.funcName,
            'line': record.lineno
        }

        if record.exc_info:
            log_data['exception'] = self.formatException(record.exc_info)

        return json.dumps(log_data, ensure_ascii=False)

# ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã«é©ç”¨
json_handler = logging.StreamHandler()
json_handler.setFormatter(JSONFormatter())
logger.addHandler(json_handler)
```

**ã‚¢ãƒ©ãƒ¼ãƒˆè¨­å®šï¼ˆRender Notificationsï¼‰**:

**Step 1: Render Dashboard > Service > Notifications**

```
1. "Add Notification" ã‚¯ãƒªãƒƒã‚¯
2. Notification Type: Email, Slack, Webhook
3. Events:
   - Deploy Started
   - Deploy Succeeded
   - Deploy Failed
   - Service Suspended (ç„¡æ–™ãƒ—ãƒ©ãƒ³)
   - High Memory Usage (80%ä»¥ä¸Š)
   - Health Check Failed
```

**Step 2: Slack Webhookçµ±åˆä¾‹**

```python
# app_improved.py - Slackã‚¢ãƒ©ãƒ¼ãƒˆé€ä¿¡

import requests

SLACK_WEBHOOK_URL = os.environ.get('SLACK_WEBHOOK_URL')

def send_slack_alert(message, level='warning'):
    """Slackã«ã‚¢ãƒ©ãƒ¼ãƒˆé€ä¿¡"""
    if not SLACK_WEBHOOK_URL:
        return

    colors = {
        'info': '#36a64f',
        'warning': '#ff9900',
        'error': '#ff0000'
    }

    payload = {
        'attachments': [{
            'color': colors.get(level, '#808080'),
            'title': f'Q-Storm Platform Alert ({level.upper()})',
            'text': message,
            'footer': 'Render Deployment',
            'ts': int(time.time())
        }]
    }

    try:
        requests.post(SLACK_WEBHOOK_URL, json=payload, timeout=5)
    except Exception as e:
        logger.error(f'Slacké€šçŸ¥å¤±æ•—: {e}')

# ä½¿ç”¨ä¾‹
send_slack_alert('Memory usage exceeded 80%', level='warning')
send_slack_alert('LangChain API quota exceeded', level='error')
```

**ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆå¼·åŒ–**:

```python
@app.route('/api/v2/health', methods=['GET'])
def health_check():
    """Renderç”¨ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ï¼ˆæ‹¡å¼µç‰ˆï¼‰"""

    health_status = {
        'status': 'healthy',
        'timestamp': datetime.now().isoformat(),
        'version': '3.0.0',
        'checks': {}
    }

    # 1. ãƒ¡ãƒ¢ãƒªãƒã‚§ãƒƒã‚¯
    try:
        import psutil
        memory = psutil.virtual_memory()
        health_status['checks']['memory'] = {
            'status': 'ok' if memory.percent < 85 else 'warning',
            'usage_percent': memory.percent,
            'available_gb': round(memory.available / (1024**3), 2)
        }
    except Exception as e:
        health_status['checks']['memory'] = {'status': 'error', 'message': str(e)}

    # 2. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šãƒã‚§ãƒƒã‚¯ï¼ˆPhase 2ï¼‰
    if DATABASE_URL:
        try:
            engine.execute('SELECT 1')
            health_status['checks']['database'] = {'status': 'ok'}
        except Exception as e:
            health_status['checks']['database'] = {'status': 'error', 'message': str(e)}
            health_status['status'] = 'degraded'

    # 3. LangChain APIãƒã‚§ãƒƒã‚¯
    api_key = os.environ.get('OPENAI_API_KEY', '')
    health_status['checks']['langchain'] = {
        'status': 'ok' if api_key else 'disabled',
        'api_key_configured': bool(api_key)
    }

    # 4. ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ãƒã‚§ãƒƒã‚¯
    try:
        disk_usage = psutil.disk_usage('/')
        health_status['checks']['disk'] = {
            'status': 'ok' if disk_usage.percent < 90 else 'warning',
            'usage_percent': disk_usage.percent
        }
    except Exception as e:
        health_status['checks']['disk'] = {'status': 'error', 'message': str(e)}

    # ç·åˆã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹åˆ¤å®š
    check_statuses = [check.get('status') for check in health_status['checks'].values()]
    if 'error' in check_statuses:
        health_status['status'] = 'unhealthy'
        status_code = 503
    elif 'warning' in check_statuses:
        health_status['status'] = 'degraded'
        status_code = 200
    else:
        health_status['status'] = 'healthy'
        status_code = 200

    return jsonify(health_status), status_code
```

**ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°**:

```python
# app_improved.py - ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ¡ãƒˆãƒªã‚¯ã‚¹

from functools import wraps
import time

request_metrics = {
    'total_requests': 0,
    'total_duration': 0,
    'endpoint_stats': {}
}

def track_performance(f):
    """ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°"""
    @wraps(f)
    def decorated_function(*args, **kwargs):
        start_time = time.time()

        try:
            response = f(*args, **kwargs)
            return response
        finally:
            duration = time.time() - start_time
            endpoint = request.endpoint or 'unknown'

            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ›´æ–°
            request_metrics['total_requests'] += 1
            request_metrics['total_duration'] += duration

            if endpoint not in request_metrics['endpoint_stats']:
                request_metrics['endpoint_stats'][endpoint] = {
                    'count': 0,
                    'total_duration': 0,
                    'avg_duration': 0
                }

            stats = request_metrics['endpoint_stats'][endpoint]
            stats['count'] += 1
            stats['total_duration'] += duration
            stats['avg_duration'] = stats['total_duration'] / stats['count']

            # é…ã„ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ãƒ­ã‚°è¨˜éŒ²ï¼ˆ3ç§’ä»¥ä¸Šï¼‰
            if duration > 3:
                logger.warning(f'Slow request: {endpoint} took {duration:.2f}s')

    return decorated_function

# å…¨ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã«é©ç”¨
@app.route('/api/v1/analysis/eda/execute', methods=['POST'])
@track_performance
def execute_eda_analysis():
    # ... (æ—¢å­˜å®Ÿè£…)
    pass

# ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
@app.route('/api/v2/metrics', methods=['GET'])
def get_metrics():
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹å–å¾—"""
    return jsonify({
        'total_requests': request_metrics['total_requests'],
        'average_duration': request_metrics['total_duration'] / max(request_metrics['total_requests'], 1),
        'endpoints': request_metrics['endpoint_stats']
    })
```

#### F.6 ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

**ãƒ‡ãƒ—ãƒ­ã‚¤å‰ã®æº–å‚™**:

- [ ] GitHubãƒªãƒã‚¸ãƒˆãƒªä½œæˆãƒ»pushå®Œäº†
- [ ] `render.yaml` é…ç½®å®Œäº†
- [ ] `requirements.txt` æ›´æ–°ï¼ˆgunicornè¿½åŠ ï¼‰
- [ ] `.gitignore` è¨­å®šï¼ˆuploads/, outputs/, .envé™¤å¤–ï¼‰
- [ ] ç’°å¢ƒå¤‰æ•°ãƒªã‚¹ãƒˆä½œæˆï¼ˆOPENAI_API_KEYç­‰ï¼‰
- [ ] ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆå®Ÿè£…ï¼ˆ`/api/v2/health`ï¼‰

**RenderåˆæœŸè¨­å®š**:

- [ ] Renderã‚¢ã‚«ã‚¦ãƒ³ãƒˆä½œæˆ
- [ ] GitHubé€£æºå®Œäº†
- [ ] Web Serviceä½œæˆï¼ˆãƒªãƒã‚¸ãƒˆãƒªé¸æŠï¼‰
- [ ] ãƒ“ãƒ«ãƒ‰ãƒ»ã‚¹ã‚¿ãƒ¼ãƒˆã‚³ãƒãƒ³ãƒ‰è¨­å®š
- [ ] ç’°å¢ƒå¤‰æ•°è¨­å®šï¼ˆæœ€ä½é™: `OPENAI_API_KEY`, `SESSION_SECRET_KEY`ï¼‰
- [ ] Auto-Deployæœ‰åŠ¹åŒ–ï¼ˆmainãƒ–ãƒ©ãƒ³ãƒï¼‰

**ãƒ‡ãƒ—ãƒ­ã‚¤å¾Œã®ç¢ºèª**:

- [ ] ãƒ“ãƒ«ãƒ‰ãƒ­ã‚°ç¢ºèªï¼ˆã‚¨ãƒ©ãƒ¼ãªã—ï¼‰
- [ ] ãƒ‡ãƒ—ãƒ­ã‚¤æˆåŠŸç¢ºèªï¼ˆç·‘ã®ãƒã‚§ãƒƒã‚¯ãƒãƒ¼ã‚¯ï¼‰
- [ ] ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯URLç¢ºèªï¼ˆ`https://qstorm-platform.onrender.com/api/v2/health`ï¼‰
- [ ] ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆï¼ˆã‚µãƒ³ãƒ—ãƒ«Excelãƒ•ã‚¡ã‚¤ãƒ«ï¼‰
- [ ] EDAå®Ÿè¡Œãƒ†ã‚¹ãƒˆï¼ˆSweetvizãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆç¢ºèªï¼‰
- [ ] LangChainçµ±åˆãƒ†ã‚¹ãƒˆï¼ˆæ—¥æœ¬èªæ´å¯Ÿç”Ÿæˆç¢ºèª or ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
- [ ] ãƒ­ã‚°ç¢ºèªï¼ˆã‚¨ãƒ©ãƒ¼ãƒ»è­¦å‘Šãªã—ï¼‰

**Phase 2ä»¥é™ã®è¿½åŠ è¨­å®š**:

- [ ] PostgreSQLãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä½œæˆ
- [ ] `DATABASE_URL` ç’°å¢ƒå¤‰æ•°è¨­å®šï¼ˆè‡ªå‹•ï¼‰
- [ ] ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œï¼ˆAlembicï¼‰
- [ ] Redisä½œæˆï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°ç”¨ - ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
- [ ] Celery Workerãƒ‡ãƒ—ãƒ­ã‚¤ï¼ˆãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã‚¿ã‚¹ã‚¯ - ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰

**ç¶™ç¶šçš„ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°**:

- [ ] Render Notificationsè¨­å®šï¼ˆEmail/Slackï¼‰
- [ ] ãƒ­ã‚°å®šæœŸç¢ºèªï¼ˆé€±1å›ï¼‰
- [ ] ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹ç¢ºèªï¼ˆ`/api/v2/metrics`ï¼‰
- [ ] ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ç¢ºèªï¼ˆPhase 2 - æ¯æ—¥è‡ªå‹•ï¼‰
- [ ] ã‚³ã‚¹ãƒˆç¢ºèªï¼ˆRender Billing Dashboardï¼‰

---

**ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚° FAQ**:

**Q1: "Application failed to respond" ã‚¨ãƒ©ãƒ¼**

A: ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆãŒæ­£ã—ãå¿œç­”ã—ã¦ã„ãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚
```bash
# ãƒ­ã‚°ç¢ºèª
render logs -s qstorm-platform --tail 100

# ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ãƒ‘ã‚¹ç¢ºèªï¼ˆrender.yamlï¼‰
healthCheckPath: /api/v2/health

# app_improved.py ã§è©²å½“ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆå®Ÿè£…ç¢ºèª
```

**Q2: ãƒ“ãƒ«ãƒ‰ãŒå¤±æ•—ã™ã‚‹**

A: `requirements.txt` ã®ä¾å­˜é–¢ä¿‚ã‚¨ãƒ©ãƒ¼ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚
```bash
# ãƒ­ãƒ¼ã‚«ãƒ«ã§å†ç¾ãƒ†ã‚¹ãƒˆ
python3 -m venv test_env
source test_env/bin/activate
pip install -r requirements.txt

# ã‚¨ãƒ©ãƒ¼ãŒå‡ºãŸå ´åˆã€è©²å½“ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’èª¿æ•´
```

**Q3: ç’°å¢ƒå¤‰æ•°ãŒèª­ã¿è¾¼ã¾ã‚Œãªã„**

A: Render Dashboardã§ç’°å¢ƒå¤‰æ•°ã‚’è¨­å®šå¾Œã€å†ãƒ‡ãƒ—ãƒ­ã‚¤ãŒå¿…è¦ã§ã™ã€‚
```bash
# Render Dashboard > Manual Deploy > "Deploy latest commit" ã‚¯ãƒªãƒƒã‚¯
```

**Q4: ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãŒå¤±æ•—ã™ã‚‹ï¼ˆ413 Payload Too Largeï¼‰**

A: Renderã®åˆ¶é™ã€ã¾ãŸã¯ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®`MAX_FILE_SIZE_MB`è¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚
```python
# app_improved.py
app.config['MAX_CONTENT_LENGTH'] = int(os.environ.get('MAX_FILE_SIZE_MB', 200)) * 1024 * 1024
```

## Appendix G: Phase 1 Security Implementation Guide

### G.1 Input Validation Scope
- API request parameter validation
- File upload size and type restrictions
- SQL injection prevention (prepared statements)
- XSS prevention in error messages

### G.2 Security Checklist for Phase 1
- [ ] Environment variable validation
- [ ] Error message sanitization
- [ ] File path traversal prevention
- [ ] Request rate limiting (basic)

### G.3 Phase 2 Security Features (Deferred)
- User authentication (JWT/OAuth)
- Role-based access control
- API key management
- Audit logging

## Appendix H: Sample Data Standardization Guide

### H.1 Repository Data Structure
```
Q-Storm-Project3/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ sample/
â”‚   â”‚   â””â”€â”€ fixed_extended_store_data_2024-FIX_kaizen_monthlyvol3.xlsx
â”‚   â””â”€â”€ README.md
```

### H.2 Data Access Configuration
```python
# config.py
import os
from pathlib import Path

BASE_DIR = Path(__file__).parent
SAMPLE_DATA_DIR = BASE_DIR / 'data' / 'sample'
SAMPLE_DATA_FILE = 'fixed_extended_store_data_2024-FIX_kaizen_monthlyvol3.xlsx'
```

### H.3 Setup Instructions
1. Clone repository
2. Place sample data in `data/sample/` directory
3. Verify data file exists: `python verify_data_prerequisites.py`

## Appendix I: Phase 2 Preparation Roadmap

### I.1 Infrastructure PoC Timeline
- Week 1-2: PostgreSQL setup and migration strategy
- Week 3-4: Redis caching implementation
- Week 5-6: Celery async task queue

### I.2 Parallel Tasks During Phase 1
- [ ] PostgreSQL schema design
- [ ] Redis cache strategy document
- [ ] Celery task definitions
- [ ] Docker Compose configuration for Phase 2

### I.3 Phase 2 PoC Environment
```yaml
# docker-compose.yml (draft)
version: '3.8'
services:
  postgres:
    image: postgres:15
  redis:
    image: redis:7
  celery:
    build: .
    command: celery -A app worker
```

---

**END OF IMPLEMENTATION WORKFLOW**

Document Version: 1.0
Generated: 2025-10-18
Review Cycle: After each sprint retrospective
Next Review: End of Sprint 3 (Week 6)
